---
title: "A Replication of Karlan and List (2007)"
author: "Shuyang Zhang"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

The experiment was carefully designed to test whether offering a matching grant increases the likelihood and amount of donations. Over 50,000 individuals who had previously donated to a liberal nonprofit organization were randomly assigned into different treatment groups. The treatments varied along three dimensions: (1) the match ratio ($1:$1, $2:$1, or $3:$1), (2) the maximum match amount ($25,000, $50,000, $100,000, or unstated), and (3) the suggested donation amount (equal to, 1.25x, or 1.5x the donor's previous highest gift).

Each letter was identical except for the paragraph describing the match, and the response card formatting. The study aimed to test if higher match rates would induce greater giving and whether these effects varied across political geography or donor history. The results offer key insights for nonprofits on designing cost-effective fundraising strategies.

This project seeks to replicate their results.

## Data

### Description

```{python}
import pandas as pd

df = pd.read_stata("blog/karlan_list_2007.dta")
df.shape
df.head()
```


:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::


### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.
<!-- 
_todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._
-->

As an ad hoc test of the randomization mechanism, I conduct a set of tests to compare the treatment and control groups on selected pre-treatment variables. The variables I test are:
- `mrm2`: Months since last donation  
- `female`: Indicator for female donors  
- `freq`: Number of prior donations  
- `amountchange`: Change in amount given  
- `hpa`: Highest previous donation  
- `years`: Number of years since initial donation  
- `couple`: Indicator for couple households  

These are not outcome variables but may affect donation behavior, so we test whether these variables differ significantly between groups.
```{python}
from scipy.stats import ttest_ind
import statsmodels.formula.api as smf
import pandas as pd

# --- T-tests ---
t_mrm2 = ttest_ind(df[df['treatment'] == 1]['mrm2'],
                   df[df['treatment'] == 0]['mrm2'],
                   nan_policy='omit')
t_female = ttest_ind(df[df['treatment'] == 1]['female'],
                     df[df['treatment'] == 0]['female'],
                     nan_policy='omit')
t_freq = ttest_ind(df[df['treatment'] == 1]['freq'],
                   df[df['treatment'] == 0]['freq'],
                   nan_policy='omit')
t_amtchg = ttest_ind(df[df['treatment'] == 1]['amountchange'],
                     df[df['treatment'] == 0]['amountchange'],
                     nan_policy='omit')
t_hpa = ttest_ind(df[df['treatment'] == 1]['hpa'],
                  df[df['treatment'] == 0]['hpa'],
                  nan_policy='omit')

t_years = ttest_ind(df[df['treatment'] == 1]['years'],
                    df[df['treatment'] == 0]['years'],
                    nan_policy='omit')

t_couple = ttest_ind(df[df['treatment'] == 1]['couple'],
                     df[df['treatment'] == 0]['couple'],
                     nan_policy='omit')

# --- OLS regressions ---
ols_mrm2 = smf.ols('mrm2 ~ treatment', data=df).fit()
ols_female = smf.ols('female ~ treatment', data=df).fit()
ols_freq = smf.ols('freq ~ treatment', data=df).fit()
ols_amtchg = smf.ols('amountchange ~ treatment', data=df).fit()
ols_hpa = smf.ols("hpa ~ treatment", data=df).fit()
ols_years = smf.ols("years ~ treatment", data=df).fit()
ols_couple = smf.ols("couple ~ treatment", data=df).fit()
# --- Assemble into a DataFrame for table ---
balance_table = pd.DataFrame({
    'Variable': ['mrm2', 'female', 'freq', 'amountchange', 'hpa', 'years', 'couple'],
    'T-stat': [
        round(t_mrm2.statistic, 4),
        round(t_female.statistic, 4),
        round(t_freq.statistic, 4),
        round(t_amtchg.statistic, 4),
        round(t_hpa.statistic, 4),
        round(t_years.statistic, 4),
        round(t_couple.statistic, 4)
    ],
    'T p-value': [
        round(t_mrm2.pvalue, 4),
        round(t_female.pvalue, 4),
        round(t_freq.pvalue, 4),
        round(t_amtchg.pvalue, 4),
        round(t_hpa.pvalue, 4),
        round(t_years.pvalue, 4),
        round(t_couple.pvalue, 4)
    ],
    'OLS coef (treatment)': [
        round(ols_mrm2.params['treatment'], 4),
        round(ols_female.params['treatment'], 4),
        round(ols_freq.params['treatment'], 4),
        round(ols_amtchg.params['treatment'], 4),
        round(ols_hpa.params['treatment'], 4),
        round(ols_years.params['treatment'], 4),
        round(ols_couple.params['treatment'], 4)
    ],
    'OLS p-value': [
        round(ols_mrm2.pvalues['treatment'], 4),
        round(ols_female.pvalues['treatment'], 4),
        round(ols_freq.pvalues['treatment'], 4),
        round(ols_amtchg.pvalues['treatment'], 4),
        round(ols_hpa.pvalues['treatment'], 4),
        round(ols_years.pvalues['treatment'], 4),
        round(ols_couple.pvalues['treatment'], 4)
    ]
})

balance_table
```

The t-tests and regressions show that none of the selected variables are significantly different between the treatment and control groups at the 5% level (all p-values > 0.05). This suggests that the randomization was successful in creating statistically balanced groups.

This step is crucial to establish causal inference credibility, and aligns with the paper's Table 1, which also shows nearly identical summary statistics across groups. Our results replicate and reinforce their findings.

## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 
<!-- 
_todo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control._
-->

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

donation_rate = df.groupby('treatment')['gave'].mean().reset_index()
donation_rate['Group'] = donation_rate['treatment'].map({0: 'Control', 1: 'Treatment'})
donation_rate['gave'] = donation_rate['gave'] * 100 


ax = sns.barplot(data=donation_rate, x='Group', y='gave')
plt.ylabel('Proportion who donated (%)')
plt.title('Donation Rate by Group')
plt.ylim(0, 5)

for i, row in donation_rate.iterrows():
    ax.text(i, row['gave'] + 0.1, f"{row['gave']:.1f}%", ha='center', va='bottom')

plt.show()
```

<!-- 
_todo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)_
-->
```{python}
from scipy.stats import ttest_ind
import statsmodels.formula.api as smf
import pandas as pd

# T-test on binary outcome (gave)
t_gave = ttest_ind(df[df['treatment'] == 1]['gave'],
                   df[df['treatment'] == 0]['gave'],
                   nan_policy='omit')

# OLS regression
ols_gave = smf.ols("gave ~ treatment", data=df).fit()

# Create a table for results
results_table = pd.DataFrame({
    'Method': ['T-test', 'OLS Regression'],
    'Statistic': [round(t_gave.statistic, 4), round(ols_gave.params['treatment'], 4)],
    'P-value': [round(t_gave.pvalue, 4), round(ols_gave.pvalues['treatment'], 4)]
})

results_table
```

Both the t-test and OLS regression yield consistent results, showing a statistically significant difference in donation rates between the treatment and control groups. The p-value in both cases is 0.002, indicating strong evidence against the null hypothesis of no difference.

The estimated effect size from the OLS model is 0.004, meaning that the treatment group was 0.4 percentage points more likely to donate than the control group. While this may seem small in absolute terms, it is statistically meaningful given the large sample size.

This suggests that offering a matching grant—even without changing the ratio—has a measurable impact on charitable behavior. Individuals respond to the presence of a match by becoming more likely to donate, reinforcing the idea that perceived leverage or validation may encourage prosocial behavior.

<!--
_todo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper._
-->
```{python}
import statsmodels.formula.api as smf

# Probit model with interactions — replicate Table 3 Column 1
probit_formula = (
    "gave ~ treatment + "
    "treatment:ratio2 + treatment:ratio3 + "
    "treatment:size25 + treatment:size50 + treatment:size100 + "
    "treatment:ask2 + treatment:ask3"
)

probit_model = smf.probit(probit_formula, data=df).fit()
coef_table = pd.DataFrame({
    'Variable': probit_model.params.index,
    'Coef': probit_model.params.values.round(4),
    'StdErr': probit_model.bse.round(4),
    'P>|z|': probit_model.pvalues.round(4)
})

coef_table


```

The Probit regression model successfully replicates the specification used in Table 3, Column 1 of the original paper. While the magnitude of the coefficients is slightly different due to estimation of latent z-scores (as opposed to marginal effects reported in the paper), the signs and patterns of significance are broadly consistent.

Specifically:
- The `treatment` effect is positive but not statistically significant in both models.
- The interaction terms for match ratio and match size are not significant.
- The `treatment * ask2` and `treatment * ask3` interactions are statistically significant at the 1% level, consistent with the original finding that suggested donation amounts influence donor responsiveness.

Therefore, this analysis supports the robustness of the original findings.

### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.
<!-- 
_todo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the "figures suggest" comment the authors make on page 8?_
-->
```{python}
from scipy.stats import ttest_ind
import pandas as pd

# Filter treatment group by different match ratios
ratio1 = df[(df['ratio'] == 1) & (df['treatment'] == 1)]
ratio2 = df[(df['ratio2'] == 1) & (df['treatment'] == 1)]
ratio3 = df[(df['ratio3'] == 1) & (df['treatment'] == 1)]

# Perform t-tests
t_ratio2 = ttest_ind(ratio2['gave'], ratio1['gave'], nan_policy='omit')
t_ratio3 = ttest_ind(ratio3['gave'], ratio1['gave'], nan_policy='omit')

# Create a table for results
t_test_results = pd.DataFrame({
    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1'],
    'T-statistic': [round(t_ratio2.statistic, 4), round(t_ratio3.statistic, 4)],
    'P-value': [round(t_ratio2.pvalue, 4), round(t_ratio3.pvalue, 4)]
})

t_test_results
```

The results from the t-tests show that the differences in donation rates between 1:1, 2:1, and 3:1 match ratios are **not statistically significant**. This supports the original paper's statement on page 8 that higher match ratios do not systematically lead to greater donation likelihood. Our findings are consistent with the paper’s Figure 2a and the authors’ interpretation.
<!-- 
_todo: Assess the same issue using a regression. Specifically, create the variable `ratio1` then regress `gave` on `ratio1`, `ratio2`, and `ratio3` (or alternatively, regress `gave` on the categorical variable `ratio`). Interpret the coefficients and their statistical precision._
-->
```{python}
import statsmodels.formula.api as smf
from scipy.stats import ttest_ind
import pandas as pd

# Regression: gave ~ ratio2 + ratio3 (1:1 as the baseline)
ols_ratio = smf.ols("gave ~ ratio2 + ratio3", data=df[df['treatment'] == 1]).fit()

# T-tests: 2:1 vs 1:1, 3:1 vs 1:1, 3:1 vs 2:1
t_ratio2 = ttest_ind(
    df[(df['ratio2'] == 1) & (df['treatment'] == 1)]['gave'],
    df[(df['ratio'] == 1) & (df['treatment'] == 1)]['gave'],
    nan_policy='omit'
)

t_ratio3 = ttest_ind(
    df[(df['ratio3'] == 1) & (df['treatment'] == 1)]['gave'],
    df[(df['ratio'] == 1) & (df['treatment'] == 1)]['gave'],
    nan_policy='omit'
)

t_3v2 = ttest_ind(
    df[(df['ratio3'] == 1) & (df['treatment'] == 1)]['gave'],
    df[(df['ratio2'] == 1) & (df['treatment'] == 1)]['gave'],
    nan_policy='omit'
)

# Output OLS regression summary
ols_result_table = pd.DataFrame({
    'Variable': ols_ratio.params.index,
    'Coefficient': ols_ratio.params.round(4),
    'Std. Error': ols_ratio.bse.round(4),
    'P-value': ols_ratio.pvalues.round(4)
})
print(ols_result_table)
# Organize all t-test results into a table
t_test_table = pd.DataFrame({
    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1', '3:1 vs 2:1'],
    'T-statistic': [
        round(t_ratio2.statistic, 4),
        round(t_ratio3.statistic, 4),
        round(t_3v2.statistic, 4)
    ],
    'P-value': [
        round(t_ratio2.pvalue, 4),
        round(t_ratio3.pvalue, 4),
        round(t_3v2.pvalue, 4)
    ]
})

t_test_table
```

The regression and t-tests show that neither the 2:1 nor 3:1 match ratios lead to statistically significant increases in donation rates compared to the 1:1 baseline. The coefficient estimates are small and not significant, and the t-tests confirm no meaningful differences.

This finding is consistent with the original paper (Karlan & List, 2007), which states on page 8 that “we do not find systematic patterns” related to match ratio. It suggests that while the presence of a match increases donations, higher match ratios do not provide additional gains.
<!-- 
_todo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios.  Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?_
-->
```{python}
# Extract coefficients
b_2v1 = ols_ratio.params['ratio2']
b_3v1 = ols_ratio.params['ratio3']

# Create a table for the differences
response_rate_diff_table = pd.DataFrame({
    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1'],
    'Estimated Difference': [round(b_2v1, 4), round(b_3v1, 4)]
})

response_rate_diff_table
```

Using the fitted coefficients from the OLS regression, we find that:
- The 2:1 match ratio group donated at a rate approximately 0.0019 higher than the 1:1 group.
- The 3:1 match ratio group donated at a rate approximately 0.0020 higher than the 1:1 group.

These differences are very small and, as shown earlier, not statistically significant. We conclude that while matching donations increase giving relative to no match, increasing the match ratio from 1:1 to 2:1 or 3:1 provides little to no additional benefit. This supports the authors' conclusion that "figures suggest" higher match ratios do not systematically increase donation behavior.

### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.
<!-- 
_todo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_
-->
```{python}
# OLS regression on full sample
ols_amount_all = smf.ols("amount ~ treatment", data=df).fit()

# T-test (optional)
from scipy.stats import ttest_ind
t_amount_all = ttest_ind(
    df[df['treatment'] == 1]['amount'],
    df[df['treatment'] == 0]['amount'],
    nan_policy='omit'
)

# Show results
amount_all_table = pd.DataFrame({
    'Model': ['OLS: all'],
    'Coefficient': [round(ols_amount_all.params['treatment'], 4)],
    'P-value': [round(ols_amount_all.pvalues['treatment'], 4)]
})

amount_all_table
```

The OLS regression on the full sample shows that the treatment group gave on average $0.15 more than the control group. The p-value is approximately 0.063, indicating marginal significance at the 10% level. This suggests that matching offers may slightly increase the average donation amount when including everyone, even non-donors.

However, since many people donate $0, this increase is driven largely by the increase in the probability of giving, not necessarily the amount conditional on giving.
<!-- 
_todo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients -- what did we learn? Does the treatment coefficient have a causal interpretation?_ 
-->
```{python}
# OLS for donors only
ols_amount_givers = smf.ols("amount ~ treatment", data=df[df['gave'] == 1]).fit()

# Summary table
amount_givers_table = pd.DataFrame({
    'Model': ['OLS: donors only'],
    'Coefficient': [round(ols_amount_givers.params['treatment'], 4)],
    'P-value': [round(ols_amount_givers.pvalues['treatment'], 4)]
})

amount_givers_table
```

Among those who donated, the treatment group gave on average \$1.67 less than the control group, though this difference is not statistically significant (p = 0.56). This suggests no meaningful treatment effect on the donation amount once someone decides to give.

Importantly, this estimate **does not have a causal interpretation**. By restricting the sample to only those who donated, we lose the benefit of random assignment. The two groups may differ in unobservable ways, and the treatment coefficient may be biased due to selection. Therefore, this regression is descriptive but not causal.
<!-- 
_todo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot._
-->
```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Filter to only people who donated
donors = df[df['gave'] == 1]

# Split into treatment and control
treatment_group = donors[donors['treatment'] == 1]
control_group = donors[donors['treatment'] == 0]

# Calculate means
mean_treatment = treatment_group['amount'].mean()
mean_control = control_group['amount'].mean()

# Plot
fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)

# Control group plot
sns.histplot(control_group['amount'], bins=30, ax=axes[0])
axes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean: {mean_control:.2f}')
axes[0].set_title('Control Group')
axes[0].set_xlabel('Donation Amount')
axes[0].legend()

# Treatment group plot
sns.histplot(treatment_group['amount'], bins=30, ax=axes[1], color='lightgreen')
axes[1].axvline(mean_treatment, color='red', linestyle='--', label=f'Mean: {mean_treatment:.2f}')
axes[1].set_title('Treatment Group')
axes[1].set_xlabel('Donation Amount')
axes[1].legend()

plt.tight_layout()
plt.show()
```

The histograms above compare the distribution of donation amounts between the treatment and control groups, including only individuals who made a donation. Each red dashed line marks the average donation within that group.

The distribution shapes are broadly similar across groups, with both being right-skewed due to a few large donations. The treatment group appears to have a slightly lower average donation amount ($43.87) compared to the control group ($45.54), consistent with the regression results in the previous section.

This visualization supports the earlier finding that while matching offers may increase the likelihood of giving, they do not significantly affect the amount donated among those who give.

## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers
<!-- 
_to do:  Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means._
-->
```{python}
import numpy as np
import matplotlib.pyplot as plt

# Set seed for reproducibility
np.random.seed(42)

# Simulate 10,000 samples from each group
n = 10000
control_draws = np.random.binomial(1, 0.018, n)
treatment_draws = np.random.binomial(1, 0.022, n)

# Compute difference at each position
differences = treatment_draws - control_draws

# Compute cumulative average of differences
cumulative_avg = np.cumsum(differences) / np.arange(1, n+1)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(cumulative_avg, label="Cumulative Avg Difference")
plt.axhline(y=0.004, color='red', linestyle='--', label='True Difference (0.004)')
plt.title("Law of Large Numbers: Simulated Cumulative Average of Treatment-Control")
plt.xlabel("Number of Simulated Pairs")
plt.ylabel("Average Difference")
plt.legend()
plt.grid(True)
plt.show()
```

This plot illustrates the Law of Large Numbers (LLN) through a simulation. We simulate 10,000 Bernoulli draws from a control group with donation probability 0.018 and a treatment group with probability 0.022. At each step, we calculate the difference between treatment and control, and track the cumulative average.

As seen in the plot, the average difference fluctuates significantly at the beginning due to randomness in small samples. However, as the number of simulated observations increases, the cumulative average stabilizes around the true population difference of 0.004 (red dashed line). This demonstrates how, with large samples, the sample mean converges to the true mean difference — a key concept behind t-tests and statistical inference.

### Central Limit Theorem
<!-- 
_to do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the "middle" of the distribution or whether it's in the "tail."_
-->
```{python}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set parameters
p_control = 0.018
p_treatment = 0.022
n_sims = 1000
sample_sizes = [50, 200, 500, 1000]

# Set random seed
np.random.seed(42)

# Plot
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.flatten()

for i, n in enumerate(sample_sizes):
    avg_diffs = []
    for _ in range(n_sims):
        control_sample = np.random.binomial(1, p_control, n)
        treatment_sample = np.random.binomial(1, p_treatment, n)
        diff = np.mean(treatment_sample) - np.mean(control_sample)
        avg_diffs.append(diff)

    sns.histplot(avg_diffs, bins=30, ax=axes[i], kde=True)
    axes[i].axvline(x=0, color='red', linestyle='--', label='Zero')
    axes[i].axvline(x=0.004, color='green', linestyle='--', label='True Diff')
    axes[i].set_title(f"Sample size = {n}")
    axes[i].legend()

plt.suptitle("Central Limit Theorem: Distribution of Mean Differences")
plt.tight_layout()
plt.show()
```

The four histograms above illustrate the Central Limit Theorem using simulated donation decisions. For each sample size (50, 200, 500, 1000), we simulate 1,000 differences in group means between a control group (p=0.018) and a treatment group (p=0.022).

As the sample size increases:
- The distribution of mean differences becomes more **bell-shaped** (normal-like)
- The spread (variance) becomes smaller
- The mean of the distribution centers closer to the true difference (0.004)

We also see that **zero is not in the middle of the distribution**, especially as n gets larger. This suggests that the difference between groups becomes **detectable** at large sample sizes, which is the intuition behind statistical significance in t-tests.



