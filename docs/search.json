[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blogs",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nShuyang Zhang\n\n\nMay 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nShuyang Zhang\n\n\nMay 4, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html",
    "href": "blogs/blog1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was carefully designed to test whether offering a matching grant increases the likelihood and amount of donations. Over 50,000 individuals who had previously donated to a liberal nonprofit organization were randomly assigned into different treatment groups. The treatments varied along three dimensions: (1) the match ratio ($1:$1, $2:$1, or $3:$1), (2) the maximum match amount ($25,000, $50,000, $100,000, or unstated), and (3) the suggested donation amount (equal to, 1.25x, or 1.5x the donor’s previous highest gift).\nEach letter was identical except for the paragraph describing the match, and the response card formatting. The study aimed to test if higher match rates would induce greater giving and whether these effects varied across political geography or donor history. The results offer key insights for nonprofits on designing cost-effective fundraising strategies.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#introduction",
    "href": "blogs/blog1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was carefully designed to test whether offering a matching grant increases the likelihood and amount of donations. Over 50,000 individuals who had previously donated to a liberal nonprofit organization were randomly assigned into different treatment groups. The treatments varied along three dimensions: (1) the match ratio ($1:$1, $2:$1, or $3:$1), (2) the maximum match amount ($25,000, $50,000, $100,000, or unstated), and (3) the suggested donation amount (equal to, 1.25x, or 1.5x the donor’s previous highest gift).\nEach letter was identical except for the paragraph describing the match, and the response card formatting. The study aimed to test if higher match rates would induce greater giving and whether these effects varied across political geography or donor history. The results offer key insights for nonprofits on designing cost-effective fundraising strategies.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#data",
    "href": "blogs/blog1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.shape\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another. \nAs an ad hoc test of the randomization mechanism, I conduct a set of tests to compare the treatment and control groups on selected pre-treatment variables. The variables I test are: - mrm2: Months since last donation\n- female: Indicator for female donors\n- freq: Number of prior donations\n- amountchange: Change in amount given\n- hpa: Highest previous donation\n- years: Number of years since initial donation\n- couple: Indicator for couple households\nThese are not outcome variables but may affect donation behavior, so we test whether these variables differ significantly between groups.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\n# --- T-tests ---\nt_mrm2 = ttest_ind(df[df['treatment'] == 1]['mrm2'],\n                   df[df['treatment'] == 0]['mrm2'],\n                   nan_policy='omit')\nt_female = ttest_ind(df[df['treatment'] == 1]['female'],\n                     df[df['treatment'] == 0]['female'],\n                     nan_policy='omit')\nt_freq = ttest_ind(df[df['treatment'] == 1]['freq'],\n                   df[df['treatment'] == 0]['freq'],\n                   nan_policy='omit')\nt_amtchg = ttest_ind(df[df['treatment'] == 1]['amountchange'],\n                     df[df['treatment'] == 0]['amountchange'],\n                     nan_policy='omit')\nt_hpa = ttest_ind(df[df['treatment'] == 1]['hpa'],\n                  df[df['treatment'] == 0]['hpa'],\n                  nan_policy='omit')\n\nt_years = ttest_ind(df[df['treatment'] == 1]['years'],\n                    df[df['treatment'] == 0]['years'],\n                    nan_policy='omit')\n\nt_couple = ttest_ind(df[df['treatment'] == 1]['couple'],\n                     df[df['treatment'] == 0]['couple'],\n                     nan_policy='omit')\n\n# --- OLS regressions ---\nols_mrm2 = smf.ols('mrm2 ~ treatment', data=df).fit()\nols_female = smf.ols('female ~ treatment', data=df).fit()\nols_freq = smf.ols('freq ~ treatment', data=df).fit()\nols_amtchg = smf.ols('amountchange ~ treatment', data=df).fit()\nols_hpa = smf.ols(\"hpa ~ treatment\", data=df).fit()\nols_years = smf.ols(\"years ~ treatment\", data=df).fit()\nols_couple = smf.ols(\"couple ~ treatment\", data=df).fit()\n# --- Assemble into a DataFrame for table ---\nbalance_table = pd.DataFrame({\n    'Variable': ['mrm2', 'female', 'freq', 'amountchange', 'hpa', 'years', 'couple'],\n    'T-stat': [\n        round(t_mrm2.statistic, 4),\n        round(t_female.statistic, 4),\n        round(t_freq.statistic, 4),\n        round(t_amtchg.statistic, 4),\n        round(t_hpa.statistic, 4),\n        round(t_years.statistic, 4),\n        round(t_couple.statistic, 4)\n    ],\n    'T p-value': [\n        round(t_mrm2.pvalue, 4),\n        round(t_female.pvalue, 4),\n        round(t_freq.pvalue, 4),\n        round(t_amtchg.pvalue, 4),\n        round(t_hpa.pvalue, 4),\n        round(t_years.pvalue, 4),\n        round(t_couple.pvalue, 4)\n    ],\n    'OLS coef (treatment)': [\n        round(ols_mrm2.params['treatment'], 4),\n        round(ols_female.params['treatment'], 4),\n        round(ols_freq.params['treatment'], 4),\n        round(ols_amtchg.params['treatment'], 4),\n        round(ols_hpa.params['treatment'], 4),\n        round(ols_years.params['treatment'], 4),\n        round(ols_couple.params['treatment'], 4)\n    ],\n    'OLS p-value': [\n        round(ols_mrm2.pvalues['treatment'], 4),\n        round(ols_female.pvalues['treatment'], 4),\n        round(ols_freq.pvalues['treatment'], 4),\n        round(ols_amtchg.pvalues['treatment'], 4),\n        round(ols_hpa.pvalues['treatment'], 4),\n        round(ols_years.pvalues['treatment'], 4),\n        round(ols_couple.pvalues['treatment'], 4)\n    ]\n})\n\nbalance_table\n\n\n\n\n\n\n\n\nVariable\nT-stat\nT p-value\nOLS coef (treatment)\nOLS p-value\n\n\n\n\n0\nmrm2\n0.1195\n0.9049\n0.0137\n0.9049\n\n\n1\nfemale\n-1.7584\n0.0787\n-0.0075\n0.0787\n\n\n2\nfreq\n-0.1109\n0.9117\n-0.0120\n0.9117\n\n\n3\namountchange\n0.5270\n0.5982\n6.3306\n0.5982\n\n\n4\nhpa\n0.9441\n0.3451\n0.6371\n0.3451\n\n\n5\nyears\n-1.1030\n0.2700\n-0.0575\n0.2700\n\n\n6\ncouple\n-0.5838\n0.5594\n-0.0016\n0.5594\n\n\n\n\n\n\n\nThe t-tests and regressions show that none of the selected variables are significantly different between the treatment and control groups at the 5% level (all p-values &gt; 0.05). This suggests that the randomization was successful in creating statistically balanced groups.\nThis step is crucial to establish causal inference credibility, and aligns with the paper’s Table 1, which also shows nearly identical summary statistics across groups. Our results replicate and reinforce their findings."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#experimental-results",
    "href": "blogs/blog1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndonation_rate = df.groupby('treatment')['gave'].mean().reset_index()\ndonation_rate['Group'] = donation_rate['treatment'].map({0: 'Control', 1: 'Treatment'})\ndonation_rate['gave'] = donation_rate['gave'] * 100 \n\n\nax = sns.barplot(data=donation_rate, x='Group', y='gave')\nplt.ylabel('Proportion who donated (%)')\nplt.title('Donation Rate by Group')\nplt.ylim(0, 5)\n\nfor i, row in donation_rate.iterrows():\n    ax.text(i, row['gave'] + 0.1, f\"{row['gave']:.1f}%\", ha='center', va='bottom')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\n# T-test on binary outcome (gave)\nt_gave = ttest_ind(df[df['treatment'] == 1]['gave'],\n                   df[df['treatment'] == 0]['gave'],\n                   nan_policy='omit')\n\n# OLS regression\nols_gave = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Create a table for results\nresults_table = pd.DataFrame({\n    'Method': ['T-test', 'OLS Regression'],\n    'Statistic': [round(t_gave.statistic, 4), round(ols_gave.params['treatment'], 4)],\n    'P-value': [round(t_gave.pvalue, 4), round(ols_gave.pvalues['treatment'], 4)]\n})\n\nresults_table\n\n\n\n\n\n\n\n\nMethod\nStatistic\nP-value\n\n\n\n\n0\nT-test\n3.1014\n0.0019\n\n\n1\nOLS Regression\n0.0042\n0.0019\n\n\n\n\n\n\n\nBoth the t-test and OLS regression yield consistent results, showing a statistically significant difference in donation rates between the treatment and control groups. The p-value in both cases is 0.002, indicating strong evidence against the null hypothesis of no difference.\nThe estimated effect size from the OLS model is 0.004, meaning that the treatment group was 0.4 percentage points more likely to donate than the control group. While this may seem small in absolute terms, it is statistically meaningful given the large sample size.\nThis suggests that offering a matching grant—even without changing the ratio—has a measurable impact on charitable behavior. Individuals respond to the presence of a match by becoming more likely to donate, reinforcing the idea that perceived leverage or validation may encourage prosocial behavior.\n\n\nimport statsmodels.formula.api as smf\n\n# Probit model with interactions — replicate Table 3 Column 1\nprobit_formula = (\n    \"gave ~ treatment + \"\n    \"treatment:ratio2 + treatment:ratio3 + \"\n    \"treatment:size25 + treatment:size50 + treatment:size100 + \"\n    \"treatment:ask2 + treatment:ask3\"\n)\n\nprobit_model = smf.probit(probit_formula, data=df).fit()\ncoef_table = pd.DataFrame({\n    'Variable': probit_model.params.index,\n    'Coef': probit_model.params.values.round(4),\n    'StdErr': probit_model.bse.round(4),\n    'P&gt;|z|': probit_model.pvalues.round(4)\n})\n\ncoef_table\n\nOptimization terminated successfully.\n         Current function value: 0.100276\n         Iterations 7\n\n\n\n\n\n\n\n\n\nVariable\nCoef\nStdErr\nP&gt;|z|\n\n\n\n\nIntercept\nIntercept\n-2.1001\n0.0233\n0.0000\n\n\ntreatment\ntreatment\n0.0656\n0.0462\n0.1558\n\n\ntreatment:ratio2\ntreatment:ratio2\n0.0370\n0.0377\n0.3266\n\n\ntreatment:ratio3\ntreatment:ratio3\n0.0398\n0.0377\n0.2914\n\n\ntreatment:size25\ntreatment:size25\n-0.0129\n0.0434\n0.7673\n\n\ntreatment:size50\ntreatment:size50\n0.0052\n0.0431\n0.9040\n\n\ntreatment:size100\ntreatment:size100\n-0.0010\n0.0432\n0.9813\n\n\ntreatment:ask2\ntreatment:ask2\n0.0198\n0.0059\n0.0008\n\n\ntreatment:ask3\ntreatment:ask3\n-0.0164\n0.0049\n0.0009\n\n\n\n\n\n\n\nThe Probit regression model successfully replicates the specification used in Table 3, Column 1 of the original paper. While the magnitude of the coefficients is slightly different due to estimation of latent z-scores (as opposed to marginal effects reported in the paper), the signs and patterns of significance are broadly consistent.\nSpecifically: - The treatment effect is positive but not statistically significant in both models. - The interaction terms for match ratio and match size are not significant. - The treatment * ask2 and treatment * ask3 interactions are statistically significant at the 1% level, consistent with the original finding that suggested donation amounts influence donor responsiveness.\nTherefore, this analysis supports the robustness of the original findings.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. \n\nfrom scipy.stats import ttest_ind\nimport pandas as pd\n\n# Filter treatment group by different match ratios\nratio1 = df[(df['ratio'] == 1) & (df['treatment'] == 1)]\nratio2 = df[(df['ratio2'] == 1) & (df['treatment'] == 1)]\nratio3 = df[(df['ratio3'] == 1) & (df['treatment'] == 1)]\n\n# Perform t-tests\nt_ratio2 = ttest_ind(ratio2['gave'], ratio1['gave'], nan_policy='omit')\nt_ratio3 = ttest_ind(ratio3['gave'], ratio1['gave'], nan_policy='omit')\n\n# Create a table for results\nt_test_results = pd.DataFrame({\n    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1'],\n    'T-statistic': [round(t_ratio2.statistic, 4), round(t_ratio3.statistic, 4)],\n    'P-value': [round(t_ratio2.pvalue, 4), round(t_ratio3.pvalue, 4)]\n})\n\nt_test_results\n\n\n\n\n\n\n\n\nComparison\nT-statistic\nP-value\n\n\n\n\n0\n2:1 vs 1:1\n0.965\n0.3345\n\n\n1\n3:1 vs 1:1\n1.015\n0.3101\n\n\n\n\n\n\n\nThe results from the t-tests show that the differences in donation rates between 1:1, 2:1, and 3:1 match ratios are not statistically significant. This supports the original paper’s statement on page 8 that higher match ratios do not systematically lead to greater donation likelihood. Our findings are consistent with the paper’s Figure 2a and the authors’ interpretation. \n\nimport statsmodels.formula.api as smf\nfrom scipy.stats import ttest_ind\nimport pandas as pd\n\n# Regression: gave ~ ratio2 + ratio3 (1:1 as the baseline)\nols_ratio = smf.ols(\"gave ~ ratio2 + ratio3\", data=df[df['treatment'] == 1]).fit()\n\n# T-tests: 2:1 vs 1:1, 3:1 vs 1:1, 3:1 vs 2:1\nt_ratio2 = ttest_ind(\n    df[(df['ratio2'] == 1) & (df['treatment'] == 1)]['gave'],\n    df[(df['ratio'] == 1) & (df['treatment'] == 1)]['gave'],\n    nan_policy='omit'\n)\n\nt_ratio3 = ttest_ind(\n    df[(df['ratio3'] == 1) & (df['treatment'] == 1)]['gave'],\n    df[(df['ratio'] == 1) & (df['treatment'] == 1)]['gave'],\n    nan_policy='omit'\n)\n\nt_3v2 = ttest_ind(\n    df[(df['ratio3'] == 1) & (df['treatment'] == 1)]['gave'],\n    df[(df['ratio2'] == 1) & (df['treatment'] == 1)]['gave'],\n    nan_policy='omit'\n)\n\n# Output OLS regression summary\nols_result_table = pd.DataFrame({\n    'Variable': ols_ratio.params.index,\n    'Coefficient': ols_ratio.params.round(4),\n    'Std. Error': ols_ratio.bse.round(4),\n    'P-value': ols_ratio.pvalues.round(4)\n})\nprint(ols_result_table)\n# Organize all t-test results into a table\nt_test_table = pd.DataFrame({\n    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1', '3:1 vs 2:1'],\n    'T-statistic': [\n        round(t_ratio2.statistic, 4),\n        round(t_ratio3.statistic, 4),\n        round(t_3v2.statistic, 4)\n    ],\n    'P-value': [\n        round(t_ratio2.pvalue, 4),\n        round(t_ratio3.pvalue, 4),\n        round(t_3v2.pvalue, 4)\n    ]\n})\n\nt_test_table\n\n            Variable  Coefficient  Std. Error  P-value\nIntercept  Intercept       0.0207      0.0014   0.0000\nratio2        ratio2       0.0019      0.0020   0.3383\nratio3        ratio3       0.0020      0.0020   0.3133\n\n\n\n\n\n\n\n\n\nComparison\nT-statistic\nP-value\n\n\n\n\n0\n2:1 vs 1:1\n0.9650\n0.3345\n\n\n1\n3:1 vs 1:1\n1.0150\n0.3101\n\n\n2\n3:1 vs 2:1\n0.0501\n0.9600\n\n\n\n\n\n\n\nThe regression and t-tests show that neither the 2:1 nor 3:1 match ratios lead to statistically significant increases in donation rates compared to the 1:1 baseline. The coefficient estimates are small and not significant, and the t-tests confirm no meaningful differences.\nThis finding is consistent with the original paper (Karlan & List, 2007), which states on page 8 that “we do not find systematic patterns” related to match ratio. It suggests that while the presence of a match increases donations, higher match ratios do not provide additional gains. \n\n# Extract coefficients\nb_2v1 = ols_ratio.params['ratio2']\nb_3v1 = ols_ratio.params['ratio3']\n\n# Create a table for the differences\nresponse_rate_diff_table = pd.DataFrame({\n    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1'],\n    'Estimated Difference': [round(b_2v1, 4), round(b_3v1, 4)]\n})\n\nresponse_rate_diff_table\n\n\n\n\n\n\n\n\nComparison\nEstimated Difference\n\n\n\n\n0\n2:1 vs 1:1\n0.0019\n\n\n1\n3:1 vs 1:1\n0.0020\n\n\n\n\n\n\n\nUsing the fitted coefficients from the OLS regression, we find that: - The 2:1 match ratio group donated at a rate approximately 0.0019 higher than the 1:1 group. - The 3:1 match ratio group donated at a rate approximately 0.0020 higher than the 1:1 group.\nThese differences are very small and, as shown earlier, not statistically significant. We conclude that while matching donations increase giving relative to no match, increasing the match ratio from 1:1 to 2:1 or 3:1 provides little to no additional benefit. This supports the authors’ conclusion that “figures suggest” higher match ratios do not systematically increase donation behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution. \n\n# OLS regression on full sample\nols_amount_all = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n# T-test (optional)\nfrom scipy.stats import ttest_ind\nt_amount_all = ttest_ind(\n    df[df['treatment'] == 1]['amount'],\n    df[df['treatment'] == 0]['amount'],\n    nan_policy='omit'\n)\n\n# Show results\namount_all_table = pd.DataFrame({\n    'Model': ['OLS: all'],\n    'Coefficient': [round(ols_amount_all.params['treatment'], 4)],\n    'P-value': [round(ols_amount_all.pvalues['treatment'], 4)]\n})\n\namount_all_table\n\n\n\n\n\n\n\n\nModel\nCoefficient\nP-value\n\n\n\n\n0\nOLS: all\n0.1536\n0.0628\n\n\n\n\n\n\n\nThe OLS regression on the full sample shows that the treatment group gave on average $0.15 more than the control group. The p-value is approximately 0.063, indicating marginal significance at the 10% level. This suggests that matching offers may slightly increase the average donation amount when including everyone, even non-donors.\nHowever, since many people donate $0, this increase is driven largely by the increase in the probability of giving, not necessarily the amount conditional on giving. \n\n# OLS for donors only\nols_amount_givers = smf.ols(\"amount ~ treatment\", data=df[df['gave'] == 1]).fit()\n\n# Summary table\namount_givers_table = pd.DataFrame({\n    'Model': ['OLS: donors only'],\n    'Coefficient': [round(ols_amount_givers.params['treatment'], 4)],\n    'P-value': [round(ols_amount_givers.pvalues['treatment'], 4)]\n})\n\namount_givers_table\n\n\n\n\n\n\n\n\nModel\nCoefficient\nP-value\n\n\n\n\n0\nOLS: donors only\n-1.6684\n0.5615\n\n\n\n\n\n\n\nAmong those who donated, the treatment group gave on average $1.67 less than the control group, though this difference is not statistically significant (p = 0.56). This suggests no meaningful treatment effect on the donation amount once someone decides to give.\nImportantly, this estimate does not have a causal interpretation. By restricting the sample to only those who donated, we lose the benefit of random assignment. The two groups may differ in unobservable ways, and the treatment coefficient may be biased due to selection. Therefore, this regression is descriptive but not causal. \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter to only people who donated\ndonors = df[df['gave'] == 1]\n\n# Split into treatment and control\ntreatment_group = donors[donors['treatment'] == 1]\ncontrol_group = donors[donors['treatment'] == 0]\n\n# Calculate means\nmean_treatment = treatment_group['amount'].mean()\nmean_control = control_group['amount'].mean()\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group plot\nsns.histplot(control_group['amount'], bins=30, ax=axes[0])\naxes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean: {mean_control:.2f}')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\naxes[0].legend()\n\n# Treatment group plot\nsns.histplot(treatment_group['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(mean_treatment, color='red', linestyle='--', label=f'Mean: {mean_treatment:.2f}')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms above compare the distribution of donation amounts between the treatment and control groups, including only individuals who made a donation. Each red dashed line marks the average donation within that group.\nThe distribution shapes are broadly similar across groups, with both being right-skewed due to a few large donations. The treatment group appears to have a slightly lower average donation amount ($43.87) compared to the control group ($45.54), consistent with the regression results in the previous section.\nThis visualization supports the earlier finding that while matching offers may increase the likelihood of giving, they do not significantly affect the amount donated among those who give."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#simulation-experiment",
    "href": "blogs/blog1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 10,000 samples from each group\nn = 10000\ncontrol_draws = np.random.binomial(1, 0.018, n)\ntreatment_draws = np.random.binomial(1, 0.022, n)\n\n# Compute difference at each position\ndifferences = treatment_draws - control_draws\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, n+1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Avg Difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers: Simulated Cumulative Average of Treatment-Control\")\nplt.xlabel(\"Number of Simulated Pairs\")\nplt.ylabel(\"Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot illustrates the Law of Large Numbers (LLN) through a simulation. We simulate 10,000 Bernoulli draws from a control group with donation probability 0.018 and a treatment group with probability 0.022. At each step, we calculate the difference between treatment and control, and track the cumulative average.\nAs seen in the plot, the average difference fluctuates significantly at the beginning due to randomness in small samples. However, as the number of simulated observations increases, the cumulative average stabilizes around the true population difference of 0.004 (red dashed line). This demonstrates how, with large samples, the sample mean converges to the true mean difference — a key concept behind t-tests and statistical inference.\n\n\nCentral Limit Theorem\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set parameters\np_control = 0.018\np_treatment = 0.022\nn_sims = 1000\nsample_sizes = [50, 200, 500, 1000]\n\n# Set random seed\nnp.random.seed(42)\n\n# Plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n    for _ in range(n_sims):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_diffs.append(diff)\n\n    sns.histplot(avg_diffs, bins=30, ax=axes[i], kde=True)\n    axes[i].axvline(x=0, color='red', linestyle='--', label='Zero')\n    axes[i].axvline(x=0.004, color='green', linestyle='--', label='True Diff')\n    axes[i].set_title(f\"Sample size = {n}\")\n    axes[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms above illustrate the Central Limit Theorem using simulated donation decisions. For each sample size (50, 200, 500, 1000), we simulate 1,000 differences in group means between a control group (p=0.018) and a treatment group (p=0.022).\nAs the sample size increases: - The distribution of mean differences becomes more bell-shaped (normal-like) - The spread (variance) becomes smaller - The mean of the distribution centers closer to the true difference (0.004)\nWe also see that zero is not in the middle of the distribution, especially as n gets larger. This suggests that the difference between groups becomes detectable at large sample sizes, which is the intuition behind statistical significance in t-tests."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shuyang Zhang",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blogs/blog2/hw2_questions.html",
    "href": "blogs/blog2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data Cleaning (using df_clean)\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]\n      .dropna(subset=['patents'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\n\npalette = [\"#4DB6AC\", \"#FFC17A\"]            # Non-customer / Customer\n\n# Grouped Bar Plot\nplt.figure(figsize=(10,6))\n\nmax_pat = int(df_clean['patents'].max())\nsns.countplot(\n    data=df_clean,\n    x='patents',\n    hue='customer_status',\n    order=list(range(0, max_pat + 1)),      # Ensure 0,1,2,… order\n    palette=palette\n)\n\nplt.xticks(np.arange(0, max_pat + 1, 1))    # Integer ticks\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"patents\"].agg([\"mean\", \"count\"]).round({\"mean\": 4})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\ncount\n\n\niscustomer\n\n\n\n\n\n\n0\n3.4730\n1019\n\n\n1\n4.1331\n481\n\n\n\n\n\n\n\nAverage patents: Firms that are Blueprinty customers hold approximately 4.13 patents on average, while non-customers hold approximately 3.47. The mean gap is about 0.7 patents, roughly 20% higher for customers.\nDistribution shape: Both groups peak around 2 to 4 patents, but the customer distribution is slightly shifted right and has a fatter right tail, with more firms holding 6 or more patents. Low-patent mass: There is a noticeably larger concentration of non-customers at 0 to 1 patent, suggesting many younger or less innovative firms do not subscribe to Blueprinty.\nOverlap: Despite the shift, the two histograms overlap heavily, indicating that many customers and non-customers share similar patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers. \n\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]          # Keep only valid values\n      .dropna(subset=['patents', 'age', 'region'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\npalette = [\"#4DB6AC\", \"#FFC17A\"]\n\n# Region Distribution\nplt.figure(figsize=(9, 4))\nsns.countplot(data=df_clean, x='region', hue='customer_status', palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\n# Contingency table (proportion of customers within each region)\ndisplay(pd.crosstab(df_clean['region'],\n                    df_clean['customer_status'],\n                    normalize='index').mul(100).round(2).astype(str) + '%')\n\n# Age Distribution \n# Age Histogram (side-by-side + centered + proper legend) \n# Discretize age into intervals\nage_bins   = [0, 10, 20, 30, 40, np.inf]                \nage_labels = ['&lt;10', '10-19', '20-29', '30-39', '40+']   \ndf_clean['age_group'] = pd.cut(df_clean['age'],\n                               bins=age_bins,\n                               labels=age_labels,\n                               right=False,           \n                               ordered=True)\n\n# Side-by-side bar plot similar to region \nplt.figure(figsize=(9,4))\nsns.countplot(data=df_clean,\n              x='age_group',\n              hue='customer_status',\n              palette=palette)\nplt.title(\"Age Group Distribution by Customer Status\")\nplt.xlabel(\"Firm Age Group (years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\npd.crosstab(df_clean['age_group'],\n            df_clean['customer_status'],\n            normalize='index').mul(100).round(2).astype(str) + '%'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nregion\n\n\n\n\n\n\nMidwest\n16.52%\n83.48%\n\n\nNortheast\n54.58%\n45.42%\n\n\nNorthwest\n15.51%\n84.49%\n\n\nSouth\n18.32%\n81.68%\n\n\nSouthwest\n17.51%\n82.49%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nage_group\n\n\n\n\n\n\n&lt;10\n0.0%\n100.0%\n\n\n10-19\n32.56%\n67.44%\n\n\n20-29\n30.04%\n69.96%\n\n\n30-39\n32.79%\n67.21%\n\n\n40+\n50.91%\n49.09%\n\n\n\n\n\n\n\nCustomers and non-customers are not evenly distributed across regions. Customers are much more concentrated in the Northeast, where over half of firms are Blueprinty users. In contrast, in all other regions, only about 16% to 18% of firms are customers.\nThis suggests region could be a confounding factor if firms in the Northeast are more innovative or patent-active.\nIn terms of firm age, customers are more likely to come from older firms. Among firms aged 40+, customers and non-customers are nearly evenly split (51% vs 49%). However, in younger groups, especially under 30 years old, customers make up only about 30% to 33%.\nThis implies that age is also a likely confounder, since older firms tend to have more patents and are more likely to adopt Blueprinty.\nTherefore, both region and age show systematic differences between customers and non-customers, supporting the need to control for these variables in further analysis. ### Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.  \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nOr in log-likelihood form:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf \n    log_likelihood = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport matplotlib.pyplot as plt\nY = df_clean[\"patents\"].values\n\nlambda_values = np.linspace(0.1, 10, 200)\nlog_likelihoods = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 4))\nplt.plot(lambda_values, log_likelihoods, color=\"green\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nlambda_mle = np.mean(Y)\nprint(\"Closed-form MLE of lambda:\", round(lambda_mle, 4))\n\nClosed-form MLE of lambda: 3.6847\n\n\n\nBy solving the first-order condition for the Poisson log-likelihood, we obtain a closed-form maximum likelihood estimate of λ equal to the sample mean:\n\nfrom scipy.optimize import minimize_scalar\nneg_loglikelihood = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method='bounded')\nprint(\"Optimized lambda (MLE):\", round(result.x, 4))\n\nOptimized lambda (MLE): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty. \n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Parameters:\n        beta : array-like (p,)\n        Y    : array-like (n,)\n        X    : array-like (n, p)\n\n    Returns:\n        scalar log-likelihood value\n    \"\"\"\n    # Compute λ_i = exp(X_i @ β) for each sample\n    linear_predictor = X @ beta\n    lambda_i = np.exp(linear_predictor)\n\n    # Prevent numerical errors (e.g., overflow)\n    if np.any(lambda_i &lt;= 0):\n        return -np.inf\n\n    # Log-likelihood: sum of [ -λ_i + y_i * log(λ_i) - log(y_i!) ]\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Design Matrix \ndf = df_clean.copy()\ndf[\"age_z\"]    = (df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()\ndf[\"age_sq_z\"] = df[\"age_z\"] ** 2\nregion_dum     = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_z\", \"age_sq_z\", \"iscustomer\"]],\n    region_dum\n], axis=1).astype(float)\n\nY = df[\"patents\"].astype(float).to_numpy()\nX = X_df.to_numpy()\n\n# Poisson Log-Likelihood \ndef poi_ll(beta, y, x):\n    eta = np.clip(x @ beta, -50, 50)          # Prevent overflow\n    lam = np.exp(eta)\n    return np.sum(-lam + y * eta - gammaln(y + 1))\n\nneg_ll = lambda b: -poi_ll(b, Y, X)\nbeta0  = np.zeros(X.shape[1])\n\n# L-BFGS-B Optimization (Silent Mode) \nopt_res = minimize(neg_ll, beta0, method=\"L-BFGS-B\",\n                   options={\"maxiter\": 1000, \"disp\": False})\n\nbeta_hat = opt_res.x\nhess_inv = opt_res.hess_inv.todense()\nse_hat   = np.sqrt(np.diag(hess_inv))\n\n# Results Table \nsummary = pd.DataFrame({\n    \"Variable\":    X_df.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\":  se_hat\n})\nsummary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.344688\n1.115057\n\n\n1\nage_z\n-0.057729\n0.897139\n\n\n2\nage_sq_z\n-0.155799\n0.360097\n\n\n3\niscustomer\n0.207595\n0.634206\n\n\n4\nregion_Northeast\n0.029131\n1.207625\n\n\n5\nregion_Northwest\n-0.017579\n1.368696\n\n\n6\nregion_South\n0.056525\n1.311021\n\n\n7\nregion_Southwest\n0.050555\n0.388660\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Fit the model\nglm_res = sm.GLM(Y, X_df, family=sm.families.Poisson()).fit()\n\n# Create results table\nsummary_df = pd.DataFrame({\n    \"Variable\": X_df.columns,\n    \"Coefficient\": glm_res.params.values,\n    \"Std. Error\": glm_res.bse.values,\n    \"z\": glm_res.tvalues.values,\n    \"P&gt;|z|\": glm_res.pvalues.values\n}).round(4)\n\n# Display as a pandas table\nsummary_df\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz\nP&gt;|z|\n\n\n\n\n0\nintercept\n1.3447\n0.0384\n35.0587\n0.0000\n\n\n1\nage_z\n-0.0577\n0.0150\n-3.8431\n0.0001\n\n\n2\nage_sq_z\n-0.1558\n0.0135\n-11.5132\n0.0000\n\n\n3\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\n6\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\nThe model predicts the expected patent count for a firm as exp(Xβ). After controlling for age and region, being a Blueprinty customer increases the expected number of patents by roughly 23 percent (exp 0.207 ≈ 1.23) and this effect is highly significant.\nFirm age has a positive coefficient, meaning older firms tend to hold more patents, but the negative age-squared term shows the marginal gain declines as firms get very old.\nNone of the region dummies are statistically significant, suggesting location has little effect after controlling for age and customer status. The positive intercept reflects the expected log number of reviews for an average-age, non-customer listing in the baseline region. Customer status remains a significant positive predictor, supporting the idea that Blueprinty users receive more reviews—though this is an observational result and does not imply causality.X \n\nX_0 = X_df.copy()\nX_1 = X_df.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\ny_pred_0 = glm_res.predict(X_0)\ny_pred_1 = glm_res.predict(X_1)\n\ndelta = y_pred_1 - y_pred_0\naverage_effect = delta.mean()\n\nprint(\"Average effect of using Blueprinty on predicted patent count:\", round(average_effect, 4))\n\nAverage effect of using Blueprinty on predicted patent count: 0.7928\n\n\nUsing the fitted Poisson regression model, we compute counterfactual predictions for each firm under two scenarios: one where no firm uses Blueprinty and one where all firms do. The average predicted increase in patent count from switching all firms to Blueprinty users is approximately 0.79 patents per firm. This result suggests that, after controlling for firm age and region, being a Blueprinty customer is associated with an average increase of 0.79 patents over the 5-year period."
  },
  {
    "objectID": "blogs/blog2/hw2_questions.html#blueprinty-case-study",
    "href": "blogs/blog2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data Cleaning (using df_clean)\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]\n      .dropna(subset=['patents'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\n\npalette = [\"#4DB6AC\", \"#FFC17A\"]            # Non-customer / Customer\n\n# Grouped Bar Plot\nplt.figure(figsize=(10,6))\n\nmax_pat = int(df_clean['patents'].max())\nsns.countplot(\n    data=df_clean,\n    x='patents',\n    hue='customer_status',\n    order=list(range(0, max_pat + 1)),      # Ensure 0,1,2,… order\n    palette=palette\n)\n\nplt.xticks(np.arange(0, max_pat + 1, 1))    # Integer ticks\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"patents\"].agg([\"mean\", \"count\"]).round({\"mean\": 4})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\ncount\n\n\niscustomer\n\n\n\n\n\n\n0\n3.4730\n1019\n\n\n1\n4.1331\n481\n\n\n\n\n\n\n\nAverage patents: Firms that are Blueprinty customers hold approximately 4.13 patents on average, while non-customers hold approximately 3.47. The mean gap is about 0.7 patents, roughly 20% higher for customers.\nDistribution shape: Both groups peak around 2 to 4 patents, but the customer distribution is slightly shifted right and has a fatter right tail, with more firms holding 6 or more patents. Low-patent mass: There is a noticeably larger concentration of non-customers at 0 to 1 patent, suggesting many younger or less innovative firms do not subscribe to Blueprinty.\nOverlap: Despite the shift, the two histograms overlap heavily, indicating that many customers and non-customers share similar patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers. \n\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]          # Keep only valid values\n      .dropna(subset=['patents', 'age', 'region'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\npalette = [\"#4DB6AC\", \"#FFC17A\"]\n\n# Region Distribution\nplt.figure(figsize=(9, 4))\nsns.countplot(data=df_clean, x='region', hue='customer_status', palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\n# Contingency table (proportion of customers within each region)\ndisplay(pd.crosstab(df_clean['region'],\n                    df_clean['customer_status'],\n                    normalize='index').mul(100).round(2).astype(str) + '%')\n\n# Age Distribution \n# Age Histogram (side-by-side + centered + proper legend) \n# Discretize age into intervals\nage_bins   = [0, 10, 20, 30, 40, np.inf]                \nage_labels = ['&lt;10', '10-19', '20-29', '30-39', '40+']   \ndf_clean['age_group'] = pd.cut(df_clean['age'],\n                               bins=age_bins,\n                               labels=age_labels,\n                               right=False,           \n                               ordered=True)\n\n# Side-by-side bar plot similar to region \nplt.figure(figsize=(9,4))\nsns.countplot(data=df_clean,\n              x='age_group',\n              hue='customer_status',\n              palette=palette)\nplt.title(\"Age Group Distribution by Customer Status\")\nplt.xlabel(\"Firm Age Group (years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\npd.crosstab(df_clean['age_group'],\n            df_clean['customer_status'],\n            normalize='index').mul(100).round(2).astype(str) + '%'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nregion\n\n\n\n\n\n\nMidwest\n16.52%\n83.48%\n\n\nNortheast\n54.58%\n45.42%\n\n\nNorthwest\n15.51%\n84.49%\n\n\nSouth\n18.32%\n81.68%\n\n\nSouthwest\n17.51%\n82.49%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nage_group\n\n\n\n\n\n\n&lt;10\n0.0%\n100.0%\n\n\n10-19\n32.56%\n67.44%\n\n\n20-29\n30.04%\n69.96%\n\n\n30-39\n32.79%\n67.21%\n\n\n40+\n50.91%\n49.09%\n\n\n\n\n\n\n\nCustomers and non-customers are not evenly distributed across regions. Customers are much more concentrated in the Northeast, where over half of firms are Blueprinty users. In contrast, in all other regions, only about 16% to 18% of firms are customers.\nThis suggests region could be a confounding factor if firms in the Northeast are more innovative or patent-active.\nIn terms of firm age, customers are more likely to come from older firms. Among firms aged 40+, customers and non-customers are nearly evenly split (51% vs 49%). However, in younger groups, especially under 30 years old, customers make up only about 30% to 33%.\nThis implies that age is also a likely confounder, since older firms tend to have more patents and are more likely to adopt Blueprinty.\nTherefore, both region and age show systematic differences between customers and non-customers, supporting the need to control for these variables in further analysis. ### Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.  \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nOr in log-likelihood form:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf \n    log_likelihood = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport matplotlib.pyplot as plt\nY = df_clean[\"patents\"].values\n\nlambda_values = np.linspace(0.1, 10, 200)\nlog_likelihoods = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 4))\nplt.plot(lambda_values, log_likelihoods, color=\"green\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nlambda_mle = np.mean(Y)\nprint(\"Closed-form MLE of lambda:\", round(lambda_mle, 4))\n\nClosed-form MLE of lambda: 3.6847\n\n\n\nBy solving the first-order condition for the Poisson log-likelihood, we obtain a closed-form maximum likelihood estimate of λ equal to the sample mean:\n\nfrom scipy.optimize import minimize_scalar\nneg_loglikelihood = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method='bounded')\nprint(\"Optimized lambda (MLE):\", round(result.x, 4))\n\nOptimized lambda (MLE): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty. \n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Parameters:\n        beta : array-like (p,)\n        Y    : array-like (n,)\n        X    : array-like (n, p)\n\n    Returns:\n        scalar log-likelihood value\n    \"\"\"\n    # Compute λ_i = exp(X_i @ β) for each sample\n    linear_predictor = X @ beta\n    lambda_i = np.exp(linear_predictor)\n\n    # Prevent numerical errors (e.g., overflow)\n    if np.any(lambda_i &lt;= 0):\n        return -np.inf\n\n    # Log-likelihood: sum of [ -λ_i + y_i * log(λ_i) - log(y_i!) ]\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Design Matrix \ndf = df_clean.copy()\ndf[\"age_z\"]    = (df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()\ndf[\"age_sq_z\"] = df[\"age_z\"] ** 2\nregion_dum     = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_z\", \"age_sq_z\", \"iscustomer\"]],\n    region_dum\n], axis=1).astype(float)\n\nY = df[\"patents\"].astype(float).to_numpy()\nX = X_df.to_numpy()\n\n# Poisson Log-Likelihood \ndef poi_ll(beta, y, x):\n    eta = np.clip(x @ beta, -50, 50)          # Prevent overflow\n    lam = np.exp(eta)\n    return np.sum(-lam + y * eta - gammaln(y + 1))\n\nneg_ll = lambda b: -poi_ll(b, Y, X)\nbeta0  = np.zeros(X.shape[1])\n\n# L-BFGS-B Optimization (Silent Mode) \nopt_res = minimize(neg_ll, beta0, method=\"L-BFGS-B\",\n                   options={\"maxiter\": 1000, \"disp\": False})\n\nbeta_hat = opt_res.x\nhess_inv = opt_res.hess_inv.todense()\nse_hat   = np.sqrt(np.diag(hess_inv))\n\n# Results Table \nsummary = pd.DataFrame({\n    \"Variable\":    X_df.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\":  se_hat\n})\nsummary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.344688\n1.115057\n\n\n1\nage_z\n-0.057729\n0.897139\n\n\n2\nage_sq_z\n-0.155799\n0.360097\n\n\n3\niscustomer\n0.207595\n0.634206\n\n\n4\nregion_Northeast\n0.029131\n1.207625\n\n\n5\nregion_Northwest\n-0.017579\n1.368696\n\n\n6\nregion_South\n0.056525\n1.311021\n\n\n7\nregion_Southwest\n0.050555\n0.388660\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Fit the model\nglm_res = sm.GLM(Y, X_df, family=sm.families.Poisson()).fit()\n\n# Create results table\nsummary_df = pd.DataFrame({\n    \"Variable\": X_df.columns,\n    \"Coefficient\": glm_res.params.values,\n    \"Std. Error\": glm_res.bse.values,\n    \"z\": glm_res.tvalues.values,\n    \"P&gt;|z|\": glm_res.pvalues.values\n}).round(4)\n\n# Display as a pandas table\nsummary_df\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz\nP&gt;|z|\n\n\n\n\n0\nintercept\n1.3447\n0.0384\n35.0587\n0.0000\n\n\n1\nage_z\n-0.0577\n0.0150\n-3.8431\n0.0001\n\n\n2\nage_sq_z\n-0.1558\n0.0135\n-11.5132\n0.0000\n\n\n3\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\n6\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\nThe model predicts the expected patent count for a firm as exp(Xβ). After controlling for age and region, being a Blueprinty customer increases the expected number of patents by roughly 23 percent (exp 0.207 ≈ 1.23) and this effect is highly significant.\nFirm age has a positive coefficient, meaning older firms tend to hold more patents, but the negative age-squared term shows the marginal gain declines as firms get very old.\nNone of the region dummies are statistically significant, suggesting location has little effect after controlling for age and customer status. The positive intercept reflects the expected log number of reviews for an average-age, non-customer listing in the baseline region. Customer status remains a significant positive predictor, supporting the idea that Blueprinty users receive more reviews—though this is an observational result and does not imply causality.X \n\nX_0 = X_df.copy()\nX_1 = X_df.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\ny_pred_0 = glm_res.predict(X_0)\ny_pred_1 = glm_res.predict(X_1)\n\ndelta = y_pred_1 - y_pred_0\naverage_effect = delta.mean()\n\nprint(\"Average effect of using Blueprinty on predicted patent count:\", round(average_effect, 4))\n\nAverage effect of using Blueprinty on predicted patent count: 0.7928\n\n\nUsing the fitted Poisson regression model, we compute counterfactual predictions for each firm under two scenarios: one where no firm uses Blueprinty and one where all firms do. The average predicted increase in patent count from switching all firms to Blueprinty users is approximately 0.79 patents per firm. This result suggests that, after controlling for firm age and region, being a Blueprinty customer is associated with an average increase of 0.79 patents over the 5-year period."
  },
  {
    "objectID": "blogs/blog2/hw2_questions.html#airbnb-case-study",
    "href": "blogs/blog2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nWe begin by selecting relevant variables from the AirBnB dataset, including price, room type, number of bedrooms and bathrooms, listing duration (days), review scores, and whether the listing is instantly bookable.\nTo prepare the data for modeling, we: - Drop rows with missing values in selected columns - Convert the instant_bookable column to a binary variable (1 if “t”, else 0) - Convert the categorical room_type into dummy variables (dropping the first to avoid multicollinearity) - Construct the design matrix X_df, including a constant intercept column and all covariates\n\nimport pandas as pd\ndf = pd.read_csv(\"airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nvars_to_use = [\n    \"number_of_reviews\", \"price\", \"room_type\", \"bedrooms\", \"bathrooms\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\n\ndf_model = df[vars_to_use].dropna().copy() \n\ndf_model[\"instant_bookable\"] = (df_model[\"instant_bookable\"] == \"t\").astype(int)\nroom_dummies = pd.get_dummies(df_model[\"room_type\"], prefix=\"room\", drop_first=True)\n\nX_df = pd.concat([\n    pd.Series(1, index=df_model.index, name=\"intercept\"),\n    df_model[[\n        \"price\", \"bedrooms\", \"bathrooms\", \"days\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"\n    ]],\n    room_dummies\n], axis=1)\nX_df = X_df.astype(float)\n\n# Y\nY = df_model[\"number_of_reviews\"].astype(int).values\nX = X_df.astype(float).values\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\n\n# Fit the model: number of reviews ~ all predictors\nmodel = sm.GLM(Y, X_df, family=sm.families.Poisson())\nresults = model.fit()\n\n# Create regression results table\nsummary_df = pd.DataFrame({\n    \"Coefficient\": results.params,\n    \"Std. Error\": results.bse,\n    \"z\": results.tvalues,\n    \"P&gt;|z|\": results.pvalues\n})\n\n# Format output (center-align + round decimals)\nstyled_table = summary_df.round(4).style.format({\n    \"Coefficient\": \"{:.4f}\",\n    \"Std. Error\": \"{:.4f}\",\n    \"z\": \"{:.2f}\",\n    \"P&gt;|z|\": \"{:.4f}\"\n}).set_table_styles([{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]) \\\n  .set_properties(**{'text-align': 'center'})\n\nstyled_table  # Output table in .qmd\n\n\n\n\n\n\n \nCoefficient\nStd. Error\nz\nP&gt;|z|\n\n\n\n\nintercept\n3.4980\n0.0161\n217.40\n0.0000\n\n\nprice\n-0.0000\n0.0000\n-2.15\n0.0315\n\n\nbedrooms\n0.0741\n0.0020\n37.20\n0.0000\n\n\nbathrooms\n-0.1177\n0.0037\n-31.39\n0.0000\n\n\ndays\n0.0001\n0.0000\n129.76\n0.0000\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n75.61\n0.0000\n\n\nreview_scores_location\n-0.0769\n0.0016\n-47.80\n0.0000\n\n\nreview_scores_value\n-0.0911\n0.0018\n-50.49\n0.0000\n\n\ninstant_bookable\n0.3459\n0.0029\n119.67\n0.0000\n\n\nroom_Private room\n-0.0105\n0.0027\n-3.85\n0.0001\n\n\nroom_Shared room\n-0.2463\n0.0086\n-28.58\n0.0000\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nWe modeled the number of reviews a listing receives using a Poisson regression with various listing features as predictors.\n\nKey findings:\n\nPrice: Has a small but statistically significant negative effect. A $1 increase in price reduces expected reviews slightly (coef = –0.0000, p = 0.0315).\nBedrooms: Positively associated with reviews (coef = 0.0741). Each additional bedroom increases expected reviews by ~7.7% (exp(0.0741) ≈ 1.077).\nBathrooms: Shows a negative effect (–0.1177), suggesting listings with more bathrooms receive fewer reviews, which may reflect unobserved confounding.\nDays (listing age): Strongly positive and highly significant, indicating that older listings accumulate more reviews over time.\nReview Scores:\n\nCleanliness is positively associated with more reviews.\nSurprisingly, location and value scores show negative associations—possibly due to rating inflation or multicollinearity.\n\nInstant Bookable: Listings that allow instant booking receive ~41% more reviews on average (coef = 0.3459; exp(0.3459) ≈ 1.41). This effect is highly significant.\nRoom Type:\n\nPrivate rooms receive slightly fewer reviews (–0.0105)\nShared rooms receive substantially fewer reviews (–0.2463), or ~22% fewer reviews (exp(–0.2463) ≈ 0.78)\n\n\n\n\nConclusion\nThe model suggests that convenience (instant booking), cleanliness, and room type are key drivers of review count. While price and rating details also matter, some results (e.g., value score) may reflect latent confounding. Overall, the findings offer practical insight into what makes a listing more “bookable” or visible on Airbnb."
  }
]