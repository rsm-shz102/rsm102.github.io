[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blogs",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nShuyang Zhang\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nShuyang Zhang\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nShuyang Zhang\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html",
    "href": "blogs/blog1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was carefully designed to test whether offering a matching grant increases the likelihood and amount of donations. Over 50,000 individuals who had previously donated to a liberal nonprofit organization were randomly assigned into different treatment groups. The treatments varied along three dimensions: (1) the match ratio ($1:$1, $2:$1, or $3:$1), (2) the maximum match amount ($25,000, $50,000, $100,000, or unstated), and (3) the suggested donation amount (equal to, 1.25x, or 1.5x the donor’s previous highest gift).\nEach letter was identical except for the paragraph describing the match, and the response card formatting. The study aimed to test if higher match rates would induce greater giving and whether these effects varied across political geography or donor history. The results offer key insights for nonprofits on designing cost-effective fundraising strategies.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#introduction",
    "href": "blogs/blog1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was carefully designed to test whether offering a matching grant increases the likelihood and amount of donations. Over 50,000 individuals who had previously donated to a liberal nonprofit organization were randomly assigned into different treatment groups. The treatments varied along three dimensions: (1) the match ratio ($1:$1, $2:$1, or $3:$1), (2) the maximum match amount ($25,000, $50,000, $100,000, or unstated), and (3) the suggested donation amount (equal to, 1.25x, or 1.5x the donor’s previous highest gift).\nEach letter was identical except for the paragraph describing the match, and the response card formatting. The study aimed to test if higher match rates would induce greater giving and whether these effects varied across political geography or donor history. The results offer key insights for nonprofits on designing cost-effective fundraising strategies.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#data",
    "href": "blogs/blog1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.shape\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another. \nAs an ad hoc test of the randomization mechanism, I conduct a set of tests to compare the treatment and control groups on selected pre-treatment variables. The variables I test are: - mrm2: Months since last donation\n- female: Indicator for female donors\n- freq: Number of prior donations\n- amountchange: Change in amount given\n- hpa: Highest previous donation\n- years: Number of years since initial donation\n- couple: Indicator for couple households\nThese are not outcome variables but may affect donation behavior, so we test whether these variables differ significantly between groups.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\n# --- T-tests ---\nt_mrm2 = ttest_ind(df[df['treatment'] == 1]['mrm2'],\n                   df[df['treatment'] == 0]['mrm2'],\n                   nan_policy='omit')\nt_female = ttest_ind(df[df['treatment'] == 1]['female'],\n                     df[df['treatment'] == 0]['female'],\n                     nan_policy='omit')\nt_freq = ttest_ind(df[df['treatment'] == 1]['freq'],\n                   df[df['treatment'] == 0]['freq'],\n                   nan_policy='omit')\nt_amtchg = ttest_ind(df[df['treatment'] == 1]['amountchange'],\n                     df[df['treatment'] == 0]['amountchange'],\n                     nan_policy='omit')\nt_hpa = ttest_ind(df[df['treatment'] == 1]['hpa'],\n                  df[df['treatment'] == 0]['hpa'],\n                  nan_policy='omit')\n\nt_years = ttest_ind(df[df['treatment'] == 1]['years'],\n                    df[df['treatment'] == 0]['years'],\n                    nan_policy='omit')\n\nt_couple = ttest_ind(df[df['treatment'] == 1]['couple'],\n                     df[df['treatment'] == 0]['couple'],\n                     nan_policy='omit')\n\n# --- OLS regressions ---\nols_mrm2 = smf.ols('mrm2 ~ treatment', data=df).fit()\nols_female = smf.ols('female ~ treatment', data=df).fit()\nols_freq = smf.ols('freq ~ treatment', data=df).fit()\nols_amtchg = smf.ols('amountchange ~ treatment', data=df).fit()\nols_hpa = smf.ols(\"hpa ~ treatment\", data=df).fit()\nols_years = smf.ols(\"years ~ treatment\", data=df).fit()\nols_couple = smf.ols(\"couple ~ treatment\", data=df).fit()\n# --- Assemble into a DataFrame for table ---\nbalance_table = pd.DataFrame({\n    'Variable': ['mrm2', 'female', 'freq', 'amountchange', 'hpa', 'years', 'couple'],\n    'T-stat': [\n        round(t_mrm2.statistic, 4),\n        round(t_female.statistic, 4),\n        round(t_freq.statistic, 4),\n        round(t_amtchg.statistic, 4),\n        round(t_hpa.statistic, 4),\n        round(t_years.statistic, 4),\n        round(t_couple.statistic, 4)\n    ],\n    'T p-value': [\n        round(t_mrm2.pvalue, 4),\n        round(t_female.pvalue, 4),\n        round(t_freq.pvalue, 4),\n        round(t_amtchg.pvalue, 4),\n        round(t_hpa.pvalue, 4),\n        round(t_years.pvalue, 4),\n        round(t_couple.pvalue, 4)\n    ],\n    'OLS coef (treatment)': [\n        round(ols_mrm2.params['treatment'], 4),\n        round(ols_female.params['treatment'], 4),\n        round(ols_freq.params['treatment'], 4),\n        round(ols_amtchg.params['treatment'], 4),\n        round(ols_hpa.params['treatment'], 4),\n        round(ols_years.params['treatment'], 4),\n        round(ols_couple.params['treatment'], 4)\n    ],\n    'OLS p-value': [\n        round(ols_mrm2.pvalues['treatment'], 4),\n        round(ols_female.pvalues['treatment'], 4),\n        round(ols_freq.pvalues['treatment'], 4),\n        round(ols_amtchg.pvalues['treatment'], 4),\n        round(ols_hpa.pvalues['treatment'], 4),\n        round(ols_years.pvalues['treatment'], 4),\n        round(ols_couple.pvalues['treatment'], 4)\n    ]\n})\n\nbalance_table\n\n\n\n\n\n\n\n\nVariable\nT-stat\nT p-value\nOLS coef (treatment)\nOLS p-value\n\n\n\n\n0\nmrm2\n0.1195\n0.9049\n0.0137\n0.9049\n\n\n1\nfemale\n-1.7584\n0.0787\n-0.0075\n0.0787\n\n\n2\nfreq\n-0.1109\n0.9117\n-0.0120\n0.9117\n\n\n3\namountchange\n0.5270\n0.5982\n6.3306\n0.5982\n\n\n4\nhpa\n0.9441\n0.3451\n0.6371\n0.3451\n\n\n5\nyears\n-1.1030\n0.2700\n-0.0575\n0.2700\n\n\n6\ncouple\n-0.5838\n0.5594\n-0.0016\n0.5594\n\n\n\n\n\n\n\nThe t-tests and regressions show that none of the selected variables are significantly different between the treatment and control groups at the 5% level (all p-values &gt; 0.05). This suggests that the randomization was successful in creating statistically balanced groups.\nThis step is crucial to establish causal inference credibility, and aligns with the paper’s Table 1, which also shows nearly identical summary statistics across groups. Our results replicate and reinforce their findings."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#experimental-results",
    "href": "blogs/blog1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndonation_rate = df.groupby('treatment')['gave'].mean().reset_index()\ndonation_rate['Group'] = donation_rate['treatment'].map({0: 'Control', 1: 'Treatment'})\ndonation_rate['gave'] = donation_rate['gave'] * 100 \n\n\nax = sns.barplot(data=donation_rate, x='Group', y='gave')\nplt.ylabel('Proportion who donated (%)')\nplt.title('Donation Rate by Group')\nplt.ylim(0, 5)\n\nfor i, row in donation_rate.iterrows():\n    ax.text(i, row['gave'] + 0.1, f\"{row['gave']:.1f}%\", ha='center', va='bottom')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\n# T-test on binary outcome (gave)\nt_gave = ttest_ind(df[df['treatment'] == 1]['gave'],\n                   df[df['treatment'] == 0]['gave'],\n                   nan_policy='omit')\n\n# OLS regression\nols_gave = smf.ols(\"gave ~ treatment\", data=df).fit()\n\n# Create a table for results\nresults_table = pd.DataFrame({\n    'Method': ['T-test', 'OLS Regression'],\n    'Statistic': [round(t_gave.statistic, 4), round(ols_gave.params['treatment'], 4)],\n    'P-value': [round(t_gave.pvalue, 4), round(ols_gave.pvalues['treatment'], 4)]\n})\n\nresults_table\n\n\n\n\n\n\n\n\nMethod\nStatistic\nP-value\n\n\n\n\n0\nT-test\n3.1014\n0.0019\n\n\n1\nOLS Regression\n0.0042\n0.0019\n\n\n\n\n\n\n\nBoth the t-test and OLS regression yield consistent results, showing a statistically significant difference in donation rates between the treatment and control groups. The p-value in both cases is 0.002, indicating strong evidence against the null hypothesis of no difference.\nThe estimated effect size from the OLS model is 0.004, meaning that the treatment group was 0.4 percentage points more likely to donate than the control group. While this may seem small in absolute terms, it is statistically meaningful given the large sample size.\nThis suggests that offering a matching grant—even without changing the ratio—has a measurable impact on charitable behavior. Individuals respond to the presence of a match by becoming more likely to donate, reinforcing the idea that perceived leverage or validation may encourage prosocial behavior.\n\n\nimport statsmodels.formula.api as smf\n\n# Probit model with interactions — replicate Table 3 Column 1\nprobit_formula = (\n    \"gave ~ treatment + \"\n    \"treatment:ratio2 + treatment:ratio3 + \"\n    \"treatment:size25 + treatment:size50 + treatment:size100 + \"\n    \"treatment:ask2 + treatment:ask3\"\n)\n\nprobit_model = smf.probit(probit_formula, data=df).fit()\ncoef_table = pd.DataFrame({\n    'Variable': probit_model.params.index,\n    'Coef': probit_model.params.values.round(4),\n    'StdErr': probit_model.bse.round(4),\n    'P&gt;|z|': probit_model.pvalues.round(4)\n})\n\ncoef_table\n\nOptimization terminated successfully.\n         Current function value: 0.100276\n         Iterations 7\n\n\n\n\n\n\n\n\n\nVariable\nCoef\nStdErr\nP&gt;|z|\n\n\n\n\nIntercept\nIntercept\n-2.1001\n0.0233\n0.0000\n\n\ntreatment\ntreatment\n0.0656\n0.0462\n0.1558\n\n\ntreatment:ratio2\ntreatment:ratio2\n0.0370\n0.0377\n0.3266\n\n\ntreatment:ratio3\ntreatment:ratio3\n0.0398\n0.0377\n0.2914\n\n\ntreatment:size25\ntreatment:size25\n-0.0129\n0.0434\n0.7673\n\n\ntreatment:size50\ntreatment:size50\n0.0052\n0.0431\n0.9040\n\n\ntreatment:size100\ntreatment:size100\n-0.0010\n0.0432\n0.9813\n\n\ntreatment:ask2\ntreatment:ask2\n0.0198\n0.0059\n0.0008\n\n\ntreatment:ask3\ntreatment:ask3\n-0.0164\n0.0049\n0.0009\n\n\n\n\n\n\n\nThe Probit regression model successfully replicates the specification used in Table 3, Column 1 of the original paper. While the magnitude of the coefficients is slightly different due to estimation of latent z-scores (as opposed to marginal effects reported in the paper), the signs and patterns of significance are broadly consistent.\nSpecifically: - The treatment effect is positive but not statistically significant in both models. - The interaction terms for match ratio and match size are not significant. - The treatment * ask2 and treatment * ask3 interactions are statistically significant at the 1% level, consistent with the original finding that suggested donation amounts influence donor responsiveness.\nTherefore, this analysis supports the robustness of the original findings.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. \n\nfrom scipy.stats import ttest_ind\nimport pandas as pd\n\n# Filter treatment group by different match ratios\nratio1 = df[(df['ratio'] == 1) & (df['treatment'] == 1)]\nratio2 = df[(df['ratio2'] == 1) & (df['treatment'] == 1)]\nratio3 = df[(df['ratio3'] == 1) & (df['treatment'] == 1)]\n\n# Perform t-tests\nt_ratio2 = ttest_ind(ratio2['gave'], ratio1['gave'], nan_policy='omit')\nt_ratio3 = ttest_ind(ratio3['gave'], ratio1['gave'], nan_policy='omit')\n\n# Create a table for results\nt_test_results = pd.DataFrame({\n    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1'],\n    'T-statistic': [round(t_ratio2.statistic, 4), round(t_ratio3.statistic, 4)],\n    'P-value': [round(t_ratio2.pvalue, 4), round(t_ratio3.pvalue, 4)]\n})\n\nt_test_results\n\n\n\n\n\n\n\n\nComparison\nT-statistic\nP-value\n\n\n\n\n0\n2:1 vs 1:1\n0.965\n0.3345\n\n\n1\n3:1 vs 1:1\n1.015\n0.3101\n\n\n\n\n\n\n\nThe results from the t-tests show that the differences in donation rates between 1:1, 2:1, and 3:1 match ratios are not statistically significant. This supports the original paper’s statement on page 8 that higher match ratios do not systematically lead to greater donation likelihood. Our findings are consistent with the paper’s Figure 2a and the authors’ interpretation. \n\nimport statsmodels.formula.api as smf\nfrom scipy.stats import ttest_ind\nimport pandas as pd\n\n# Regression: gave ~ ratio2 + ratio3 (1:1 as the baseline)\nols_ratio = smf.ols(\"gave ~ ratio2 + ratio3\", data=df[df['treatment'] == 1]).fit()\n\n# T-tests: 2:1 vs 1:1, 3:1 vs 1:1, 3:1 vs 2:1\nt_ratio2 = ttest_ind(\n    df[(df['ratio2'] == 1) & (df['treatment'] == 1)]['gave'],\n    df[(df['ratio'] == 1) & (df['treatment'] == 1)]['gave'],\n    nan_policy='omit'\n)\n\nt_ratio3 = ttest_ind(\n    df[(df['ratio3'] == 1) & (df['treatment'] == 1)]['gave'],\n    df[(df['ratio'] == 1) & (df['treatment'] == 1)]['gave'],\n    nan_policy='omit'\n)\n\nt_3v2 = ttest_ind(\n    df[(df['ratio3'] == 1) & (df['treatment'] == 1)]['gave'],\n    df[(df['ratio2'] == 1) & (df['treatment'] == 1)]['gave'],\n    nan_policy='omit'\n)\n\n# Output OLS regression summary\nols_result_table = pd.DataFrame({\n    'Variable': ols_ratio.params.index,\n    'Coefficient': ols_ratio.params.round(4),\n    'Std. Error': ols_ratio.bse.round(4),\n    'P-value': ols_ratio.pvalues.round(4)\n})\nprint(ols_result_table)\n# Organize all t-test results into a table\nt_test_table = pd.DataFrame({\n    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1', '3:1 vs 2:1'],\n    'T-statistic': [\n        round(t_ratio2.statistic, 4),\n        round(t_ratio3.statistic, 4),\n        round(t_3v2.statistic, 4)\n    ],\n    'P-value': [\n        round(t_ratio2.pvalue, 4),\n        round(t_ratio3.pvalue, 4),\n        round(t_3v2.pvalue, 4)\n    ]\n})\n\nt_test_table\n\n            Variable  Coefficient  Std. Error  P-value\nIntercept  Intercept       0.0207      0.0014   0.0000\nratio2        ratio2       0.0019      0.0020   0.3383\nratio3        ratio3       0.0020      0.0020   0.3133\n\n\n\n\n\n\n\n\n\nComparison\nT-statistic\nP-value\n\n\n\n\n0\n2:1 vs 1:1\n0.9650\n0.3345\n\n\n1\n3:1 vs 1:1\n1.0150\n0.3101\n\n\n2\n3:1 vs 2:1\n0.0501\n0.9600\n\n\n\n\n\n\n\nThe regression and t-tests show that neither the 2:1 nor 3:1 match ratios lead to statistically significant increases in donation rates compared to the 1:1 baseline. The coefficient estimates are small and not significant, and the t-tests confirm no meaningful differences.\nThis finding is consistent with the original paper (Karlan & List, 2007), which states on page 8 that “we do not find systematic patterns” related to match ratio. It suggests that while the presence of a match increases donations, higher match ratios do not provide additional gains. \n\n# Extract coefficients\nb_2v1 = ols_ratio.params['ratio2']\nb_3v1 = ols_ratio.params['ratio3']\n\n# Create a table for the differences\nresponse_rate_diff_table = pd.DataFrame({\n    'Comparison': ['2:1 vs 1:1', '3:1 vs 1:1'],\n    'Estimated Difference': [round(b_2v1, 4), round(b_3v1, 4)]\n})\n\nresponse_rate_diff_table\n\n\n\n\n\n\n\n\nComparison\nEstimated Difference\n\n\n\n\n0\n2:1 vs 1:1\n0.0019\n\n\n1\n3:1 vs 1:1\n0.0020\n\n\n\n\n\n\n\nUsing the fitted coefficients from the OLS regression, we find that: - The 2:1 match ratio group donated at a rate approximately 0.0019 higher than the 1:1 group. - The 3:1 match ratio group donated at a rate approximately 0.0020 higher than the 1:1 group.\nThese differences are very small and, as shown earlier, not statistically significant. We conclude that while matching donations increase giving relative to no match, increasing the match ratio from 1:1 to 2:1 or 3:1 provides little to no additional benefit. This supports the authors’ conclusion that “figures suggest” higher match ratios do not systematically increase donation behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution. \n\n# OLS regression on full sample\nols_amount_all = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n# T-test (optional)\nfrom scipy.stats import ttest_ind\nt_amount_all = ttest_ind(\n    df[df['treatment'] == 1]['amount'],\n    df[df['treatment'] == 0]['amount'],\n    nan_policy='omit'\n)\n\n# Show results\namount_all_table = pd.DataFrame({\n    'Model': ['OLS: all'],\n    'Coefficient': [round(ols_amount_all.params['treatment'], 4)],\n    'P-value': [round(ols_amount_all.pvalues['treatment'], 4)]\n})\n\namount_all_table\n\n\n\n\n\n\n\n\nModel\nCoefficient\nP-value\n\n\n\n\n0\nOLS: all\n0.1536\n0.0628\n\n\n\n\n\n\n\nThe OLS regression on the full sample shows that the treatment group gave on average $0.15 more than the control group. The p-value is approximately 0.063, indicating marginal significance at the 10% level. This suggests that matching offers may slightly increase the average donation amount when including everyone, even non-donors.\nHowever, since many people donate $0, this increase is driven largely by the increase in the probability of giving, not necessarily the amount conditional on giving. \n\n# OLS for donors only\nols_amount_givers = smf.ols(\"amount ~ treatment\", data=df[df['gave'] == 1]).fit()\n\n# Summary table\namount_givers_table = pd.DataFrame({\n    'Model': ['OLS: donors only'],\n    'Coefficient': [round(ols_amount_givers.params['treatment'], 4)],\n    'P-value': [round(ols_amount_givers.pvalues['treatment'], 4)]\n})\n\namount_givers_table\n\n\n\n\n\n\n\n\nModel\nCoefficient\nP-value\n\n\n\n\n0\nOLS: donors only\n-1.6684\n0.5615\n\n\n\n\n\n\n\nAmong those who donated, the treatment group gave on average $1.67 less than the control group, though this difference is not statistically significant (p = 0.56). This suggests no meaningful treatment effect on the donation amount once someone decides to give.\nImportantly, this estimate does not have a causal interpretation. By restricting the sample to only those who donated, we lose the benefit of random assignment. The two groups may differ in unobservable ways, and the treatment coefficient may be biased due to selection. Therefore, this regression is descriptive but not causal. \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter to only people who donated\ndonors = df[df['gave'] == 1]\n\n# Split into treatment and control\ntreatment_group = donors[donors['treatment'] == 1]\ncontrol_group = donors[donors['treatment'] == 0]\n\n# Calculate means\nmean_treatment = treatment_group['amount'].mean()\nmean_control = control_group['amount'].mean()\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group plot\nsns.histplot(control_group['amount'], bins=30, ax=axes[0])\naxes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean: {mean_control:.2f}')\naxes[0].set_title('Control Group')\naxes[0].set_xlabel('Donation Amount')\naxes[0].legend()\n\n# Treatment group plot\nsns.histplot(treatment_group['amount'], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(mean_treatment, color='red', linestyle='--', label=f'Mean: {mean_treatment:.2f}')\naxes[1].set_title('Treatment Group')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms above compare the distribution of donation amounts between the treatment and control groups, including only individuals who made a donation. Each red dashed line marks the average donation within that group.\nThe distribution shapes are broadly similar across groups, with both being right-skewed due to a few large donations. The treatment group appears to have a slightly lower average donation amount ($43.87) compared to the control group ($45.54), consistent with the regression results in the previous section.\nThis visualization supports the earlier finding that while matching offers may increase the likelihood of giving, they do not significantly affect the amount donated among those who give."
  },
  {
    "objectID": "blogs/blog1/hw1_questions.html#simulation-experiment",
    "href": "blogs/blog1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 10,000 samples from each group\nn = 10000\ncontrol_draws = np.random.binomial(1, 0.018, n)\ntreatment_draws = np.random.binomial(1, 0.022, n)\n\n# Compute difference at each position\ndifferences = treatment_draws - control_draws\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, n+1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Avg Difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers: Simulated Cumulative Average of Treatment-Control\")\nplt.xlabel(\"Number of Simulated Pairs\")\nplt.ylabel(\"Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot illustrates the Law of Large Numbers (LLN) through a simulation. We simulate 10,000 Bernoulli draws from a control group with donation probability 0.018 and a treatment group with probability 0.022. At each step, we calculate the difference between treatment and control, and track the cumulative average.\nAs seen in the plot, the average difference fluctuates significantly at the beginning due to randomness in small samples. However, as the number of simulated observations increases, the cumulative average stabilizes around the true population difference of 0.004 (red dashed line). This demonstrates how, with large samples, the sample mean converges to the true mean difference — a key concept behind t-tests and statistical inference.\n\n\nCentral Limit Theorem\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set parameters\np_control = 0.018\np_treatment = 0.022\nn_sims = 1000\nsample_sizes = [50, 200, 500, 1000]\n\n# Set random seed\nnp.random.seed(42)\n\n# Plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    avg_diffs = []\n    for _ in range(n_sims):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = np.mean(treatment_sample) - np.mean(control_sample)\n        avg_diffs.append(diff)\n\n    sns.histplot(avg_diffs, bins=30, ax=axes[i], kde=True)\n    axes[i].axvline(x=0, color='red', linestyle='--', label='Zero')\n    axes[i].axvline(x=0.004, color='green', linestyle='--', label='True Diff')\n    axes[i].set_title(f\"Sample size = {n}\")\n    axes[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe four histograms above illustrate the Central Limit Theorem using simulated donation decisions. For each sample size (50, 200, 500, 1000), we simulate 1,000 differences in group means between a control group (p=0.018) and a treatment group (p=0.022).\nAs the sample size increases: - The distribution of mean differences becomes more bell-shaped (normal-like) - The spread (variance) becomes smaller - The mean of the distribution centers closer to the true difference (0.004)\nWe also see that zero is not in the middle of the distribution, especially as n gets larger. This suggests that the difference between groups becomes detectable at large sample sizes, which is the intuition behind statistical significance in t-tests."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shuyang Zhang",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blogs/blog2/hw2_questions.html",
    "href": "blogs/blog2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data Cleaning (using df_clean)\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]\n      .dropna(subset=['patents'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\n\npalette = [\"#4DB6AC\", \"#FFC17A\"]            # Non-customer / Customer\n\n# Grouped Bar Plot\nplt.figure(figsize=(10,6))\n\nmax_pat = int(df_clean['patents'].max())\nsns.countplot(\n    data=df_clean,\n    x='patents',\n    hue='customer_status',\n    order=list(range(0, max_pat + 1)),      # Ensure 0,1,2,… order\n    palette=palette\n)\n\nplt.xticks(np.arange(0, max_pat + 1, 1))    # Integer ticks\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"patents\"].agg([\"mean\", \"count\"]).round({\"mean\": 4})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\ncount\n\n\niscustomer\n\n\n\n\n\n\n0\n3.4730\n1019\n\n\n1\n4.1331\n481\n\n\n\n\n\n\n\nAverage patents: Firms that are Blueprinty customers hold approximately 4.13 patents on average, while non-customers hold approximately 3.47. The mean gap is about 0.7 patents, roughly 20% higher for customers.\nDistribution shape: Both groups peak around 2 to 4 patents, but the customer distribution is slightly shifted right and has a fatter right tail, with more firms holding 6 or more patents. Low-patent mass: There is a noticeably larger concentration of non-customers at 0 to 1 patent, suggesting many younger or less innovative firms do not subscribe to Blueprinty.\nOverlap: Despite the shift, the two histograms overlap heavily, indicating that many customers and non-customers share similar patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers. \n\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]          # Keep only valid values\n      .dropna(subset=['patents', 'age', 'region'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\npalette = [\"#4DB6AC\", \"#FFC17A\"]\n\n# Region Distribution\nplt.figure(figsize=(9, 4))\nsns.countplot(data=df_clean, x='region', hue='customer_status', palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\n# Contingency table (proportion of customers within each region)\ndisplay(pd.crosstab(df_clean['region'],\n                    df_clean['customer_status'],\n                    normalize='index').mul(100).round(2).astype(str) + '%')\n\n# Age Distribution \n# Age Histogram (side-by-side + centered + proper legend) \n# Discretize age into intervals\nage_bins   = [0, 10, 20, 30, 40, np.inf]                \nage_labels = ['&lt;10', '10-19', '20-29', '30-39', '40+']   \ndf_clean['age_group'] = pd.cut(df_clean['age'],\n                               bins=age_bins,\n                               labels=age_labels,\n                               right=False,           \n                               ordered=True)\n\n# Side-by-side bar plot similar to region \nplt.figure(figsize=(9,4))\nsns.countplot(data=df_clean,\n              x='age_group',\n              hue='customer_status',\n              palette=palette)\nplt.title(\"Age Group Distribution by Customer Status\")\nplt.xlabel(\"Firm Age Group (years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\npd.crosstab(df_clean['age_group'],\n            df_clean['customer_status'],\n            normalize='index').mul(100).round(2).astype(str) + '%'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nregion\n\n\n\n\n\n\nMidwest\n16.52%\n83.48%\n\n\nNortheast\n54.58%\n45.42%\n\n\nNorthwest\n15.51%\n84.49%\n\n\nSouth\n18.32%\n81.68%\n\n\nSouthwest\n17.51%\n82.49%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nage_group\n\n\n\n\n\n\n&lt;10\n0.0%\n100.0%\n\n\n10-19\n32.56%\n67.44%\n\n\n20-29\n30.04%\n69.96%\n\n\n30-39\n32.79%\n67.21%\n\n\n40+\n50.91%\n49.09%\n\n\n\n\n\n\n\nCustomers and non-customers are not evenly distributed across regions. Customers are much more concentrated in the Northeast, where over half of firms are Blueprinty users. In contrast, in all other regions, only about 16% to 18% of firms are customers.\nThis suggests region could be a confounding factor if firms in the Northeast are more innovative or patent-active.\nIn terms of firm age, customers are more likely to come from older firms. Among firms aged 40+, customers and non-customers are nearly evenly split (51% vs 49%). However, in younger groups, especially under 30 years old, customers make up only about 30% to 33%.\nThis implies that age is also a likely confounder, since older firms tend to have more patents and are more likely to adopt Blueprinty.\nTherefore, both region and age show systematic differences between customers and non-customers, supporting the need to control for these variables in further analysis.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.  \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nOr in log-likelihood form:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf \n    log_likelihood = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport matplotlib.pyplot as plt\nY = df_clean[\"patents\"].values\n\nlambda_values = np.linspace(0.1, 10, 200)\nlog_likelihoods = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 4))\nplt.plot(lambda_values, log_likelihoods, color=\"green\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nlambda_mle = np.mean(Y)\nprint(\"Closed-form MLE of lambda:\", round(lambda_mle, 4))\n\nClosed-form MLE of lambda: 3.6847\n\n\n\nBy solving the first-order condition for the Poisson log-likelihood, we obtain a closed-form maximum likelihood estimate of λ equal to the sample mean:\n\nfrom scipy.optimize import minimize_scalar\nneg_loglikelihood = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method='bounded')\nprint(\"Optimized lambda (MLE):\", round(result.x, 4))\n\nOptimized lambda (MLE): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty. \n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Parameters:\n        beta : array-like (p,)\n        Y    : array-like (n,)\n        X    : array-like (n, p)\n\n    Returns:\n        scalar log-likelihood value\n    \"\"\"\n    # Compute λ_i = exp(X_i @ β) for each sample\n    linear_predictor = X @ beta\n    lambda_i = np.exp(linear_predictor)\n\n    # Prevent numerical errors (e.g., overflow)\n    if np.any(lambda_i &lt;= 0):\n        return -np.inf\n\n    # Log-likelihood: sum of [ -λ_i + y_i * log(λ_i) - log(y_i!) ]\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Design Matrix \ndf = df_clean.copy()\ndf[\"age_z\"]    = (df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()\ndf[\"age_sq_z\"] = df[\"age_z\"] ** 2\nregion_dum     = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_z\", \"age_sq_z\", \"iscustomer\"]],\n    region_dum\n], axis=1).astype(float)\n\nY = df[\"patents\"].astype(float).to_numpy()\nX = X_df.to_numpy()\n\n# Poisson Log-Likelihood \ndef poi_ll(beta, y, x):\n    eta = np.clip(x @ beta, -50, 50)          # Prevent overflow\n    lam = np.exp(eta)\n    return np.sum(-lam + y * eta - gammaln(y + 1))\n\nneg_ll = lambda b: -poi_ll(b, Y, X)\nbeta0  = np.zeros(X.shape[1])\n\n# L-BFGS-B Optimization (Silent Mode) \nopt_res = minimize(neg_ll, beta0, method=\"L-BFGS-B\",\n                   options={\"maxiter\": 1000, \"disp\": False})\n\nbeta_hat = opt_res.x\nhess_inv = opt_res.hess_inv.todense()\nse_hat   = np.sqrt(np.diag(hess_inv))\n\n# Results Table \nsummary = pd.DataFrame({\n    \"Variable\":    X_df.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\":  se_hat\n})\nsummary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.344688\n1.115057\n\n\n1\nage_z\n-0.057729\n0.897139\n\n\n2\nage_sq_z\n-0.155799\n0.360097\n\n\n3\niscustomer\n0.207595\n0.634206\n\n\n4\nregion_Northeast\n0.029131\n1.207625\n\n\n5\nregion_Northwest\n-0.017579\n1.368696\n\n\n6\nregion_South\n0.056525\n1.311021\n\n\n7\nregion_Southwest\n0.050555\n0.388660\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Fit the model\nglm_res = sm.GLM(Y, X_df, family=sm.families.Poisson()).fit()\n\n# Create results table\nsummary_df = pd.DataFrame({\n    \"Variable\": X_df.columns,\n    \"Coefficient\": glm_res.params.values,\n    \"Std. Error\": glm_res.bse.values,\n    \"z\": glm_res.tvalues.values,\n    \"P&gt;|z|\": glm_res.pvalues.values\n}).round(4)\n\n# Display as a pandas table\nsummary_df\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz\nP&gt;|z|\n\n\n\n\n0\nintercept\n1.3447\n0.0384\n35.0587\n0.0000\n\n\n1\nage_z\n-0.0577\n0.0150\n-3.8431\n0.0001\n\n\n2\nage_sq_z\n-0.1558\n0.0135\n-11.5132\n0.0000\n\n\n3\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\n6\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\nThe model predicts the expected patent count for a firm as exp(Xβ). After controlling for age and region, being a Blueprinty customer increases the expected number of patents by roughly 23 percent (exp 0.207 ≈ 1.23) and this effect is highly significant.\nFirm age has a positive coefficient, meaning older firms tend to hold more patents, but the negative age-squared term shows the marginal gain declines as firms get very old.\nNone of the region dummies are statistically significant, suggesting location has little effect after controlling for age and customer status. The positive intercept reflects the expected log number of reviews for an average-age, non-customer listing in the baseline region. Customer status remains a significant positive predictor, supporting the idea that Blueprinty users receive more reviews—though this is an observational result and does not imply causality.X \n\nX_0 = X_df.copy()\nX_1 = X_df.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\ny_pred_0 = glm_res.predict(X_0)\ny_pred_1 = glm_res.predict(X_1)\n\ndelta = y_pred_1 - y_pred_0\naverage_effect = delta.mean()\n\nprint(\"Average effect of using Blueprinty on predicted patent count:\", round(average_effect, 4))\n\nAverage effect of using Blueprinty on predicted patent count: 0.7928\n\n\nUsing the fitted Poisson regression model, we compute counterfactual predictions for each firm under two scenarios: one where no firm uses Blueprinty and one where all firms do. The average predicted increase in patent count from switching all firms to Blueprinty users is approximately 0.79 patents per firm. This result suggests that, after controlling for firm age and region, being a Blueprinty customer is associated with an average increase of 0.79 patents over the 5-year period."
  },
  {
    "objectID": "blogs/blog2/hw2_questions.html#blueprinty-case-study",
    "href": "blogs/blog2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data Cleaning (using df_clean)\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]\n      .dropna(subset=['patents'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\n\npalette = [\"#4DB6AC\", \"#FFC17A\"]            # Non-customer / Customer\n\n# Grouped Bar Plot\nplt.figure(figsize=(10,6))\n\nmax_pat = int(df_clean['patents'].max())\nsns.countplot(\n    data=df_clean,\n    x='patents',\n    hue='customer_status',\n    order=list(range(0, max_pat + 1)),      # Ensure 0,1,2,… order\n    palette=palette\n)\n\nplt.xticks(np.arange(0, max_pat + 1, 1))    # Integer ticks\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"patents\"].agg([\"mean\", \"count\"]).round({\"mean\": 4})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\ncount\n\n\niscustomer\n\n\n\n\n\n\n0\n3.4730\n1019\n\n\n1\n4.1331\n481\n\n\n\n\n\n\n\nAverage patents: Firms that are Blueprinty customers hold approximately 4.13 patents on average, while non-customers hold approximately 3.47. The mean gap is about 0.7 patents, roughly 20% higher for customers.\nDistribution shape: Both groups peak around 2 to 4 patents, but the customer distribution is slightly shifted right and has a fatter right tail, with more firms holding 6 or more patents. Low-patent mass: There is a noticeably larger concentration of non-customers at 0 to 1 patent, suggesting many younger or less innovative firms do not subscribe to Blueprinty.\nOverlap: Despite the shift, the two histograms overlap heavily, indicating that many customers and non-customers share similar patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers. \n\ndf_clean = (\n    df[df['iscustomer'].isin([0, 1])]          # Keep only valid values\n      .dropna(subset=['patents', 'age', 'region'])\n      .copy()\n)\ndf_clean['customer_status'] = df_clean['iscustomer'].map({1: \"Customer\", 0: \"Non-customer\"})\npalette = [\"#4DB6AC\", \"#FFC17A\"]\n\n# Region Distribution\nplt.figure(figsize=(9, 4))\nsns.countplot(data=df_clean, x='region', hue='customer_status', palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\n# Contingency table (proportion of customers within each region)\ndisplay(pd.crosstab(df_clean['region'],\n                    df_clean['customer_status'],\n                    normalize='index').mul(100).round(2).astype(str) + '%')\n\n# Age Distribution \n# Age Histogram (side-by-side + centered + proper legend) \n# Discretize age into intervals\nage_bins   = [0, 10, 20, 30, 40, np.inf]                \nage_labels = ['&lt;10', '10-19', '20-29', '30-39', '40+']   \ndf_clean['age_group'] = pd.cut(df_clean['age'],\n                               bins=age_bins,\n                               labels=age_labels,\n                               right=False,           \n                               ordered=True)\n\n# Side-by-side bar plot similar to region \nplt.figure(figsize=(9,4))\nsns.countplot(data=df_clean,\n              x='age_group',\n              hue='customer_status',\n              palette=palette)\nplt.title(\"Age Group Distribution by Customer Status\")\nplt.xlabel(\"Firm Age Group (years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\nplt.show()\n\npd.crosstab(df_clean['age_group'],\n            df_clean['customer_status'],\n            normalize='index').mul(100).round(2).astype(str) + '%'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nregion\n\n\n\n\n\n\nMidwest\n16.52%\n83.48%\n\n\nNortheast\n54.58%\n45.42%\n\n\nNorthwest\n15.51%\n84.49%\n\n\nSouth\n18.32%\n81.68%\n\n\nSouthwest\n17.51%\n82.49%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_status\nCustomer\nNon-customer\n\n\nage_group\n\n\n\n\n\n\n&lt;10\n0.0%\n100.0%\n\n\n10-19\n32.56%\n67.44%\n\n\n20-29\n30.04%\n69.96%\n\n\n30-39\n32.79%\n67.21%\n\n\n40+\n50.91%\n49.09%\n\n\n\n\n\n\n\nCustomers and non-customers are not evenly distributed across regions. Customers are much more concentrated in the Northeast, where over half of firms are Blueprinty users. In contrast, in all other regions, only about 16% to 18% of firms are customers.\nThis suggests region could be a confounding factor if firms in the Northeast are more innovative or patent-active.\nIn terms of firm age, customers are more likely to come from older firms. Among firms aged 40+, customers and non-customers are nearly evenly split (51% vs 49%). However, in younger groups, especially under 30 years old, customers make up only about 30% to 33%.\nThis implies that age is also a likely confounder, since older firms tend to have more patents and are more likely to adopt Blueprinty.\nTherefore, both region and age show systematic differences between customers and non-customers, supporting the need to control for these variables in further analysis.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.  \\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nOr in log-likelihood form:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf \n    log_likelihood = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport matplotlib.pyplot as plt\nY = df_clean[\"patents\"].values\n\nlambda_values = np.linspace(0.1, 10, 200)\nlog_likelihoods = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_values]\n\nplt.figure(figsize=(8, 4))\nplt.plot(lambda_values, log_likelihoods, color=\"green\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nlambda_mle = np.mean(Y)\nprint(\"Closed-form MLE of lambda:\", round(lambda_mle, 4))\n\nClosed-form MLE of lambda: 3.6847\n\n\n\nBy solving the first-order condition for the Poisson log-likelihood, we obtain a closed-form maximum likelihood estimate of λ equal to the sample mean:\n\nfrom scipy.optimize import minimize_scalar\nneg_loglikelihood = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method='bounded')\nprint(\"Optimized lambda (MLE):\", round(result.x, 4))\n\nOptimized lambda (MLE): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty. \n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Parameters:\n        beta : array-like (p,)\n        Y    : array-like (n,)\n        X    : array-like (n, p)\n\n    Returns:\n        scalar log-likelihood value\n    \"\"\"\n    # Compute λ_i = exp(X_i @ β) for each sample\n    linear_predictor = X @ beta\n    lambda_i = np.exp(linear_predictor)\n\n    # Prevent numerical errors (e.g., overflow)\n    if np.any(lambda_i &lt;= 0):\n        return -np.inf\n\n    # Log-likelihood: sum of [ -λ_i + y_i * log(λ_i) - log(y_i!) ]\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Design Matrix \ndf = df_clean.copy()\ndf[\"age_z\"]    = (df[\"age\"] - df[\"age\"].mean()) / df[\"age\"].std()\ndf[\"age_sq_z\"] = df[\"age_z\"] ** 2\nregion_dum     = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_z\", \"age_sq_z\", \"iscustomer\"]],\n    region_dum\n], axis=1).astype(float)\n\nY = df[\"patents\"].astype(float).to_numpy()\nX = X_df.to_numpy()\n\n# Poisson Log-Likelihood \ndef poi_ll(beta, y, x):\n    eta = np.clip(x @ beta, -50, 50)          # Prevent overflow\n    lam = np.exp(eta)\n    return np.sum(-lam + y * eta - gammaln(y + 1))\n\nneg_ll = lambda b: -poi_ll(b, Y, X)\nbeta0  = np.zeros(X.shape[1])\n\n# L-BFGS-B Optimization (Silent Mode) \nopt_res = minimize(neg_ll, beta0, method=\"L-BFGS-B\",\n                   options={\"maxiter\": 1000, \"disp\": False})\n\nbeta_hat = opt_res.x\nhess_inv = opt_res.hess_inv.todense()\nse_hat   = np.sqrt(np.diag(hess_inv))\n\n# Results Table \nsummary = pd.DataFrame({\n    \"Variable\":    X_df.columns,\n    \"Coefficient\": beta_hat,\n    \"Std. Error\":  se_hat\n})\nsummary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.344688\n1.115057\n\n\n1\nage_z\n-0.057729\n0.897139\n\n\n2\nage_sq_z\n-0.155799\n0.360097\n\n\n3\niscustomer\n0.207595\n0.634206\n\n\n4\nregion_Northeast\n0.029131\n1.207625\n\n\n5\nregion_Northwest\n-0.017579\n1.368696\n\n\n6\nregion_South\n0.056525\n1.311021\n\n\n7\nregion_Southwest\n0.050555\n0.388660\n\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Fit the model\nglm_res = sm.GLM(Y, X_df, family=sm.families.Poisson()).fit()\n\n# Create results table\nsummary_df = pd.DataFrame({\n    \"Variable\": X_df.columns,\n    \"Coefficient\": glm_res.params.values,\n    \"Std. Error\": glm_res.bse.values,\n    \"z\": glm_res.tvalues.values,\n    \"P&gt;|z|\": glm_res.pvalues.values\n}).round(4)\n\n# Display as a pandas table\nsummary_df\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz\nP&gt;|z|\n\n\n\n\n0\nintercept\n1.3447\n0.0384\n35.0587\n0.0000\n\n\n1\nage_z\n-0.0577\n0.0150\n-3.8431\n0.0001\n\n\n2\nage_sq_z\n-0.1558\n0.0135\n-11.5132\n0.0000\n\n\n3\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\n4\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\n5\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\n6\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\n7\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\nThe model predicts the expected patent count for a firm as exp(Xβ). After controlling for age and region, being a Blueprinty customer increases the expected number of patents by roughly 23 percent (exp 0.207 ≈ 1.23) and this effect is highly significant.\nFirm age has a positive coefficient, meaning older firms tend to hold more patents, but the negative age-squared term shows the marginal gain declines as firms get very old.\nNone of the region dummies are statistically significant, suggesting location has little effect after controlling for age and customer status. The positive intercept reflects the expected log number of reviews for an average-age, non-customer listing in the baseline region. Customer status remains a significant positive predictor, supporting the idea that Blueprinty users receive more reviews—though this is an observational result and does not imply causality.X \n\nX_0 = X_df.copy()\nX_1 = X_df.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\ny_pred_0 = glm_res.predict(X_0)\ny_pred_1 = glm_res.predict(X_1)\n\ndelta = y_pred_1 - y_pred_0\naverage_effect = delta.mean()\n\nprint(\"Average effect of using Blueprinty on predicted patent count:\", round(average_effect, 4))\n\nAverage effect of using Blueprinty on predicted patent count: 0.7928\n\n\nUsing the fitted Poisson regression model, we compute counterfactual predictions for each firm under two scenarios: one where no firm uses Blueprinty and one where all firms do. The average predicted increase in patent count from switching all firms to Blueprinty users is approximately 0.79 patents per firm. This result suggests that, after controlling for firm age and region, being a Blueprinty customer is associated with an average increase of 0.79 patents over the 5-year period."
  },
  {
    "objectID": "blogs/blog2/hw2_questions.html#airbnb-case-study",
    "href": "blogs/blog2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nWe begin by selecting relevant variables from the AirBnB dataset, including price, room type, number of bedrooms and bathrooms, listing duration (days), review scores, and whether the listing is instantly bookable.\nTo prepare the data for modeling, we: - Drop rows with missing values in selected columns - Convert the instant_bookable column to a binary variable (1 if “t”, else 0) - Convert the categorical room_type into dummy variables (dropping the first to avoid multicollinearity) - Construct the design matrix X_df, including a constant intercept column and all covariates\n\nimport pandas as pd\ndf = pd.read_csv(\"airbnb.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\nvars_to_use = [\n    \"number_of_reviews\", \"price\", \"room_type\", \"bedrooms\", \"bathrooms\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\n\ndf_model = df[vars_to_use].dropna().copy() \n\ndf_model[\"instant_bookable\"] = (df_model[\"instant_bookable\"] == \"t\").astype(int)\nroom_dummies = pd.get_dummies(df_model[\"room_type\"], prefix=\"room\", drop_first=True)\n\nX_df = pd.concat([\n    pd.Series(1, index=df_model.index, name=\"intercept\"),\n    df_model[[\n        \"price\", \"bedrooms\", \"bathrooms\", \"days\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"\n    ]],\n    room_dummies\n], axis=1)\nX_df = X_df.astype(float)\n\n# Y\nY = df_model[\"number_of_reviews\"].astype(int).values\nX = X_df.astype(float).values\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\n\n# Fit the model: number of reviews ~ all predictors\nmodel = sm.GLM(Y, X_df, family=sm.families.Poisson())\nresults = model.fit()\n\n# Create regression results table\nsummary_df = pd.DataFrame({\n    \"Coefficient\": results.params,\n    \"Std. Error\": results.bse,\n    \"z\": results.tvalues,\n    \"P&gt;|z|\": results.pvalues\n})\n\n# Format output (center-align + round decimals)\nstyled_table = summary_df.round(4).style.format({\n    \"Coefficient\": \"{:.4f}\",\n    \"Std. Error\": \"{:.4f}\",\n    \"z\": \"{:.2f}\",\n    \"P&gt;|z|\": \"{:.4f}\"\n}).set_table_styles([{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]) \\\n  .set_properties(**{'text-align': 'center'})\n\nstyled_table  # Output table in .qmd\n\n\n\n\n\n\n \nCoefficient\nStd. Error\nz\nP&gt;|z|\n\n\n\n\nintercept\n3.4980\n0.0161\n217.40\n0.0000\n\n\nprice\n-0.0000\n0.0000\n-2.15\n0.0315\n\n\nbedrooms\n0.0741\n0.0020\n37.20\n0.0000\n\n\nbathrooms\n-0.1177\n0.0037\n-31.39\n0.0000\n\n\ndays\n0.0001\n0.0000\n129.76\n0.0000\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n75.61\n0.0000\n\n\nreview_scores_location\n-0.0769\n0.0016\n-47.80\n0.0000\n\n\nreview_scores_value\n-0.0911\n0.0018\n-50.49\n0.0000\n\n\ninstant_bookable\n0.3459\n0.0029\n119.67\n0.0000\n\n\nroom_Private room\n-0.0105\n0.0027\n-3.85\n0.0001\n\n\nroom_Shared room\n-0.2463\n0.0086\n-28.58\n0.0000\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nWe modeled the number of reviews a listing receives using a Poisson regression with various listing features as predictors.\n\nKey findings:\n\nPrice: Has a small but statistically significant negative effect. A $1 increase in price reduces expected reviews slightly (coef = –0.0000, p = 0.0315).\nBedrooms: Positively associated with reviews (coef = 0.0741). Each additional bedroom increases expected reviews by ~7.7% (exp(0.0741) ≈ 1.077).\nBathrooms: Shows a negative effect (–0.1177), suggesting listings with more bathrooms receive fewer reviews, which may reflect unobserved confounding.\nDays (listing age): Strongly positive and highly significant, indicating that older listings accumulate more reviews over time.\nReview Scores:\n\nCleanliness is positively associated with more reviews.\nSurprisingly, location and value scores show negative associations—possibly due to rating inflation or multicollinearity.\n\nInstant Bookable: Listings that allow instant booking receive ~41% more reviews on average (coef = 0.3459; exp(0.3459) ≈ 1.41). This effect is highly significant.\nRoom Type:\n\nPrivate rooms receive slightly fewer reviews (–0.0105)\nShared rooms receive substantially fewer reviews (–0.2463), or ~22% fewer reviews (exp(–0.2463) ≈ 0.78)\n\n\n\n\nConclusion\nThe model suggests that convenience (instant booking), cleanliness, and room type are key drivers of review count. While price and rating details also matter, some results (e.g., value score) may reflect latent confounding. Overall, the findings offer practical insight into what makes a listing more “bookable” or visible on Airbnb."
  },
  {
    "objectID": "blogs/blog3/hw3_questions.html",
    "href": "blogs/blog3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blogs/blog3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blogs/blog3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blogs/blog3/hw3_questions.html#simulate-conjoint-data",
    "href": "blogs/blog3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\nimport pandas as pd\n\nconjoint_data = pd.read_csv(\"conjoint_data.csv\")\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nbrand\nad\nprice\n\n\n\n\n0\n1\n1\n1\nN\nYes\n28\n\n\n1\n1\n1\n0\nH\nYes\n16\n\n\n2\n1\n1\n0\nP\nYes\n16\n\n\n3\n1\n2\n0\nN\nYes\n32\n\n\n4\n1\n2\n1\nP\nYes\n16\n\n\n\n\n\n\n\n::::"
  },
  {
    "objectID": "blogs/blog3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blogs/blog3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables. \n\n# One-hot encode brand variable (Hulu is baseline, so drop_first=True)\nconjoint_data = pd.get_dummies(conjoint_data, columns=[\"brand\"], drop_first=True)\nconjoint_data[\"ad\"] = conjoint_data[\"ad\"].map({\"Yes\": 1, \"No\": 0})\nconjoint_data[\"alt_id\"] = conjoint_data.groupby([\"resp\", \"task\"]).cumcount()\n\n# Reorder columns for clarity\ncols = [\"resp\", \"task\", \"alt_id\", \"choice\", \"brand_N\", \"brand_P\", \"ad\", \"price\"]\nconjoint_data = conjoint_data[cols]\n\n# Preview result\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nalt_id\nchoice\nbrand_N\nbrand_P\nad\nprice\n\n\n\n\n0\n1\n1\n0\n1\nTrue\nFalse\n1\n28\n\n\n1\n1\n1\n1\n0\nFalse\nFalse\n1\n16\n\n\n2\n1\n1\n2\n0\nFalse\nTrue\n1\n16\n\n\n3\n1\n2\n0\n0\nTrue\nFalse\n1\n32\n\n\n4\n1\n2\n1\n1\nFalse\nTrue\n1\n16"
  },
  {
    "objectID": "blogs/blog3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blogs/blog3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Step 1: Define log-likelihood function\ndef neg_log_likelihood(beta, df):\n    v = (\n        beta[0] * df[\"brand_N\"] + \n        beta[1] * df[\"brand_P\"] + \n        beta[2] * df[\"ad\"] + \n        beta[3] * df[\"price\"]\n    )\n    \n    # Compute exponentiated utility for softmax denominator\n    df[\"exp_v\"] = np.exp(v)\n    \n    # Compute denominator per (resp, task)\n    df[\"sum_exp_v\"] = df.groupby([\"resp\", \"task\"])[\"exp_v\"].transform(\"sum\")\n    \n    # Compute predicted probabilities\n    df[\"prob\"] = df[\"exp_v\"] / df[\"sum_exp_v\"]\n    \n    # Compute log-likelihood only on chosen alternatives (choice == 1)\n    df[\"log_prob\"] = np.log(df[\"prob\"])\n    ll = df.loc[df[\"choice\"] == 1, \"log_prob\"].sum()\n    \n    return -ll  # return negative log-likelihood for minimization\n\n\n\n# Step 2: Estimate parameters\ninitial_beta = np.zeros(4)  # [beta_netflix, beta_prime, beta_ads, beta_price]\nresult = minimize(neg_log_likelihood, initial_beta, args=(conjoint_data,), method='BFGS')\n\n# Step 3: Extract estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv  # estimated variance-covariance matrix\nse = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nz = 1.96\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\n# Display results\nfor i, name in enumerate([\"brand_N (Netflix)\", \"brand_P (Prime)\", \"ad\", \"price\"]):\n    print(f\"{name:20} β = {beta_hat[i]:.4f}, SE = {se[i]:.4f}, 95% CI = ({ci_lower[i]:.4f}, {ci_upper[i]:.4f})\")\n\nbrand_N (Netflix)    β = 0.9412, SE = 0.1188, 95% CI = (0.7083, 1.1741)\nbrand_P (Prime)      β = 0.5016, SE = 0.1215, 95% CI = (0.2636, 0.7397)\nad                   β = -0.7320, SE = 0.0887, 95% CI = (-0.9058, -0.5582)\nprice                β = -0.0995, SE = 0.0063, 95% CI = (-0.1119, -0.0870)"
  },
  {
    "objectID": "blogs/blog3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blogs/blog3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n# 1. Read & preprocess data\ndf = pd.read_csv(\"conjoint_data.csv\")\n\n# brand: Use Hulu as the baseline, generate brand_N / brand_P\ndf = pd.get_dummies(df, columns=[\"brand\"], drop_first=True)\n\n# ad: Map Yes → 1, No → 0\ndf[\"ad\"] = df[\"ad\"].map({\"Yes\": 1, \"No\": 0})\n\n# Standardize price to avoid overflow in softmax\ndf[\"price_std\"] = (df[\"price\"] - df[\"price\"].mean()) / df[\"price\"].std()\n\n# Assign alternative IDs (0, 1, 2) within each respondent-task group\ndf[\"alt_id\"] = df.groupby([\"resp\", \"task\"]).cumcount()\n\n# 2. Negative log-likelihood (log-sum-exp version)\ndef neg_ll(beta):\n    v = (\n        beta[0] * df[\"brand_N\"].values +\n        beta[1] * df[\"brand_P\"].values +\n        beta[2] * df[\"ad\"].values +\n        beta[3] * df[\"price_std\"].values\n    )\n    df[\"v\"] = v\n\n    # log-sum-exp trick\n    v_max   = df.groupby([\"resp\", \"task\"])[\"v\"].transform(\"max\").values\n    df[\"exp_v\"] = np.exp(v - v_max)\n\n    denom = df.groupby([\"resp\", \"task\"])[\"exp_v\"].transform(\"sum\").values\n    prob  = df[\"exp_v\"].values / denom\n\n    ll = np.log(prob[df[\"choice\"].values == 1]).sum()\n    return -ll\n\n# 3. MLE: Estimate β, Hessian → SE & 95% CI\ninit = np.zeros(4)\nmle_res = minimize(neg_ll, init, method=\"BFGS\")\nbeta_mle = mle_res.x\nvcov     = mle_res.hess_inv        # Inverse Hessian\nse_mle   = np.sqrt(np.diag(vcov))\nz        = 1.96\nci_mle   = np.vstack([beta_mle - z*se_mle, beta_mle + z*se_mle]).T\n\nmle_table = pd.DataFrame({\n    \"Parameter\": [\"β_brand_N (Netflix)\",\n                  \"β_brand_P (Prime)\",\n                  \"β_ad\",\n                  \"β_price_std\"],\n    \"β_hat\":  beta_mle.round(3),\n    \"SE\":     se_mle.round(3),\n    \"CI_low\":  ci_mle[:, 0].round(3),\n    \"CI_high\": ci_mle[:, 1].round(3)\n})\n\nmle_table.style.set_caption(\"Maximum-Likelihood Estimates\")\n\n\n\n\n\n\nTable 1: Maximum-Likelihood Estimates\n\n\n\n\n\n \nParameter\nβ_hat\nSE\nCI_low\nCI_high\n\n\n\n\n0\nβ_brand_N (Netflix)\n0.941000\n0.112000\n0.722000\n1.161000\n\n\n1\nβ_brand_P (Prime)\n0.502000\n0.113000\n0.281000\n0.722000\n\n\n2\nβ_ad\n-0.732000\n0.089000\n-0.906000\n-0.558000\n\n\n3\nβ_price_std\n-0.794000\n0.050000\n-0.893000\n-0.695000\n\n\n\n\n\n\n\n\n\n# 4. Bayesian M-H sampler\ndef log_prior(beta):\n    # N(0,5) for dummies, N(0,1) for price_std\n    sigma2 = np.array([5.0, 5.0, 5.0, 1.0])\n    return -0.5 * (np.log(2*np.pi*sigma2).sum() + np.sum(beta**2 / sigma2))\n\ndef log_post(beta):\n    return -neg_ll(beta) + log_prior(beta)\n\nn_iter, burn_in = 15_000, 3_000\nstep_sd = np.array([0.08, 0.08, 0.08, 0.04])\nrng = np.random.default_rng()\n\nsamples = np.zeros((n_iter - burn_in, 4))\nbeta_cur, logp_cur = beta_mle.copy(), log_post(beta_mle)\naccept = 0\n\nfor t in range(n_iter):\n    beta_prop = beta_cur + rng.normal(scale=step_sd)\n    logp_prop = log_post(beta_prop)\n\n    if np.log(rng.random()) &lt; (logp_prop - logp_cur):\n        beta_cur, logp_cur = beta_prop, logp_prop\n        accept += 1\n    if t &gt;= burn_in:\n        samples[t - burn_in] = beta_cur\n\nacc_rate = accept / n_iter\n\nmeans = samples.mean(axis=0)\nci_bayes = np.percentile(samples, [2.5, 97.5], axis=0).T\n\nprint(f\"Acceptance rate = {acc_rate:.2%}\\n\")\n\nbayes_table = pd.DataFrame({\n    \"Parameter\": [\"β_brand_N (Netflix)\",\n                  \"β_brand_P (Prime)\",\n                  \"β_ad\",\n                  \"β_price_std\"],\n    \"Posterior Mean\": means.round(3),\n    \"CI_low\":  ci_bayes[:, 0].round(3),\n    \"CI_high\": ci_bayes[:, 1].round(3)\n})\n\nbayes_table.style.set_caption(\"Bayesian MCMC Posterior Summary\")\n\nAcceptance rate = 43.84%\n\n\n\n\n\n\n\n\nTable 2: Bayesian MCMC Posterior Summary\n\n\n\n\n\n \nParameter\nPosterior Mean\nCI_low\nCI_high\n\n\n\n\n0\nβ_brand_N (Netflix)\n0.942000\n0.712000\n1.175000\n\n\n1\nβ_brand_P (Prime)\n0.500000\n0.284000\n0.719000\n\n\n2\nβ_ad\n-0.740000\n-0.914000\n-0.566000\n\n\n3\nβ_price_std\n-0.796000\n-0.900000\n-0.699000\n\n\n\n\n\n\n\n\n\n\n# Trace plot\nimport matplotlib.pyplot as plt\n\nplt.plot(samples[:, 0])          \nplt.xlabel(\"Iteration\")\nplt.ylabel(\"β_brand_N\")\nplt.title(\"Trace Plot – β_brand_N\")\nplt.tight_layout()\nplt.show()\n\n# Posterior histogram\nplt.hist(samples[:, 0], bins=40, density=True)\nplt.xlabel(\"β_brand_N\")\nplt.ylabel(\"Density\")\nplt.title(\"Posterior Distribution – β_brand_N\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\nparam_names = [\"β_brand_N\", \"β_brand_P\", \"β_ad\", \"β_price_std\"]\n\npost_means = samples.mean(axis=0)\npost_sds   = samples.std(axis=0, ddof=1)\npost_ci    = np.percentile(samples, [2.5, 97.5], axis=0).T\n\nsummary = pd.DataFrame({\n    \"Parameter\"     : param_names,\n    \"Post_Mean\"     : np.round(post_means, 3),\n    \"Post_SD\"       : np.round(post_sds,   3),\n    \"Post_95%_Low\"  : np.round(post_ci[:,0], 3),\n    \"Post_95%_High\" : np.round(post_ci[:,1], 3),\n    \"MLE\"           : np.round(beta_mle,   3),\n    \"MLE_SE\"        : np.round(se_mle,     3)\n})\n\nsummary\n\n\n\n\n\n\n\n\nParameter\nPost_Mean\nPost_SD\nPost_95%_Low\nPost_95%_High\nMLE\nMLE_SE\n\n\n\n\n0\nβ_brand_N\n0.942\n0.119\n0.712\n1.175\n0.941\n0.112\n\n\n1\nβ_brand_P\n0.500\n0.111\n0.284\n0.719\n0.502\n0.113\n\n\n2\nβ_ad\n-0.740\n0.088\n-0.914\n-0.566\n-0.732\n0.089\n\n\n3\nβ_price_std\n-0.796\n0.051\n-0.900\n-0.699\n-0.794\n0.050"
  },
  {
    "objectID": "blogs/blog3/hw3_questions.html#discussion",
    "href": "blogs/blog3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nBrand effects\nThe posterior mean for \\(β_{\\text{Netflix}} ≈ 0.94\\) is almost twice that of \\(β_{\\text{Prime}} ≈ 0.50\\) (both relative to the Hulu baseline).\n\nConsumers assign the highest intrinsic utility to Netflix, a moderate premium to Prime, and the least to Hulu\n\nThis ranking aligns with common market perceptions: Netflix is the category leader, Prime has a decent catalogue bundled with Prime Shipping, and Hulu is viewed as the lower-tier option\n\nAdvertising disutility\n\\(β_{\\text{ads}} ≈ -0.73\\) indicates a sizeable penalty when a plan contains ads.\n\nThe estimate is negative (as expected) and roughly comparable in magnitude to the brand premium for Prime — i.e., having ads “undoes” the benefit of moving from Hulu to Prime\n\nPrice sensitivity\n\\(β_{\\text{price\\_std}} ≈ -0.79\\) is the effect of a one–standard-deviation increase in price (≈ $6.8 in this data).\n\nConverting back to raw dollars, the marginal dis-utility is about \\(-0.12\\) per $1\n\nA negative coefficient is economically sensible; the magnitude is in line with published willingness-to-pay estimates for streaming subscriptions\n\nOverall, even without knowing the data-generation process, we would report that respondents most prefer Netflix, tolerate Prime, strongly dislike ads, and are price sensitive in the expected direction.\n\n\nMoving to a Multi-level (Random-parameters) MNL Model\nInstead of a single \\(\\beta\\) for the whole sample, assume each respondent \\(i\\) has their own \\(\\beta_i\\) drawn from a population distribution.\nSimulate such data\n\nDraw \\(\\beta_i\\) for every respondent from \\(\\mathcal{N}(\\mu, \\Sigma)\\)\nSimulate each choice task with that respondent-specific \\(\\beta_i\\) This creates realistic heterogeneity in tastes.\n\nEstimate the model\n\nThe likelihood now requires integrating over the unobserved \\(\\beta_i\\)\n\nIn practice, we approximate that integral using simulation draws (e.g. Halton sequences) and maximize a simulated log-likelihood, or take a fully Bayesian route and run a hierarchical MCMC sampler\n\nThe parameters of interest become the population mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\) (plus, optionally, posterior draws of individual \\(\\beta_i\\))\n\nWhy bother?\n\nThe multi-level model delivers not only average preferences but also the spread and correlation of those preferences across the population\n\nThat extra layer of information enables richer managerial outputs—e.g., distributional willingness-to-pay, segment-specific market-share forecasts, and more realistic demand simulations for “real-world” conjoint studies\n\nIn short, the single change is: replace the single-\\(\\beta\\) likelihood with a simulated (or hierarchical) likelihood that accounts for respondent-level heterogeneity; everything else—data structure, dummy coding, log-sum-exp numerics—remains essentially the same."
  }
]